
so, priorities now:
- read rocket propulsion elements
- Switch major to mechanical engineering, and get into a machine/hardware shop. Ask for contacts for students that are actually building hardware products for customers, from scratch. I remember Khue's friend used to do that.
- file:///C:/Users/Lenovo-1/Downloads/Ebook/Warehouse%20Management_%20A%20Complete%20Guide%20to%20Improving%20Efficiency%20and%20Minimizing%20Costs%20in%20the%20Modern%20Warehouse-Kogan%20Page%20(2014).pdf
- file:///C:/Users/Lenovo-1/Downloads/Christoph%20Roser%20-%20_Faster,%20Better,%20Cheaper_%20in%20the%20History%20of%20Manufacturing_%20From%20the%20Stone%20Age%20to%20Lean%20Manufacturing%20and%20Beyond-Productivity%20Press%20(2016).pdf
- book: "pmi-pmbok", "pmbok guide"

hey ynes. Damn you, I'm crying. Am I trading away things that I shouldn't do? I am trading away my youth, I'm trading away my freedom, my sleep time, very much my relationships <gets cut off>
oh and, just really want you to know that I really appreciate you just hanging around me for this long and just being a total nutcase (:v). Really gives a tasty flavor to my life and almost gives me smiles everyday

read Homo Deus: A Brief History of Tomorrow, just to take in the concrete ideas of UBI? Reviews said that the latter half is good

file:///home/kelvin/Downloads/James%20Edward%20Gordon%20-%20Structures_%20or,%20Why%20things%20don't%20fall%20down-Penguin%20Books%20(1978).pdf
7. joints, fastenings and people - also about creep and chariot wheels
8. soft materials and living structures - or how to design a worm
9. walls, arches and dams - or cloud towers and the stability of masonry
10. something about bridges - or saint... and saint...
11. advantage of being a beam - with observations on roofs, trusses, and masts
12. mystery of shear and torsion - or polaris and the bias-cut nightie
13. the various ways of failing in compression - or sandwiches, skulls, and dr euler
14. philosophy of design - or the shape, the weight and the cost
15. chapter of accidents - a study in sin, error, and metal fatigue
16. efficiency and aesthetic - or the world we have to live in

check robinhood's name, cuz it's currently kel, not my legal name. Yeah contact them and say things are not right. Hasn't answered yet. This is starting to look worrying

phase out magento certificates? After I get magento working, or at least the standalone simple financial model working

masterclass.comds

chapter 11 of the bankruptcy code

also connect with that girl who used to do lstm stuff, Nhu right?, and see what she has been up to lately

effective altruism global:
- https://www.eaglobal.org/events/ea-global-2017-uk/
- https://www.youtube.com/channel/UCEfASxwPxzsHlG5Rf1-4K9w

get to the angel thingy
Drexel University - Baiada Startup Incubator
skillshare, entrepreneurship hustle: from business plan to real success
also planet (company) now offers unrivalled access and rapid response to any part of the world. See their plans
do things with planets-only.net
210, 23rd. 248, section 12b, 22nd
reason about cpu-gpu communication speed and determine if training on multiple gpus are really worth it

can RL agents always be reasonable when framerate switches to 30fps to 60 or higher? If they are influenced by this then it seems like the error signal attenuation still applies right? So at that point, why not just create a much simpler policy-only optimization process?

also, pay very, very close attention to human's failure modes as well, as I don't think this has been developed in the community yet

again x3, I think I can implement this whole thing without the formulation of action-state values or state values. So try out that kind of simple tree search. Also I think by creating all of the infrastructure on my own, I can really provide a new perspective in the game

tiny bit of math and workable code: https://towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2
env: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py
more complex running code: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html

look up q learning on youtube or something

also see how humans actually do RL too. How can they correlate long distance reward signals? Are they even good at such a task? It seems like I can somehow use attention as this mechanism that spans multiple time frames, so as to make the RL capabilities more pronounced, as that acts kinda like a skip connection

so the model-based RL agents seems interesting, because it can do fundamental experiments in the world to build an accurate model, then use that model as a proxy for the real world, then do experiments on them, instead of on the real world

do the predicting distances by being consistent thingy, because it's a real possibility that may reveal more what's really going on

https://www.techrepublic.com/article/natural-language-processing-a-cheat-sheet/

how do ppl actually build chatbots from language models? In other words, how to use language models flexibly and in 1 domain. "how chatbots are made"

"neural turing machine" sounds very, very impressive, so look into that













also, spiking NNs seems interesting, so dive in that as well. Note that this is actually quite controversial

what does it mean for bert to be bidirectional? I mean, aren't attention mechanism don't have a direction?

superglue: a stickier benchmark for general-purpose language understanding systems: https://w4ngatang.github.io/static/papers/superglue.pdf
original glue benchmark: https://openreview.net/pdf?id=rJ4km2R5t7

"feature attribution, I guess they compute gradients all the way to the input and use that to construct heatmaps of which voxels are most important for a classification". Actually, the heatmaps can be generated with the last CNN layer before the activation. Anyway, bunch of links:
- https://distill.pub/2020/attribution-baselines/
- https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview
- (2018) https://mlconf.com/sessions/interpretability-beyond-feature-attribution-quant/
- https://deep.ghost.io/simple-feature-attribution/
- https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d
- interpretability beyond feature attribution: quantitative testing with concept activation vectors: https://arxiv.org/pdf/1711.11279.pdf

"robustify black-box controllers from neural nets"

the early phase of NN training: https://arxiv.org/pdf/2002.10365.pdf
deep RL from human preferences: https://arxiv.org/pdf/1706.03741.pdf, sounds familiar
the lottery ticket hypothesis: finding sparse, trainable NNs: https://arxiv.org/pdf/1803.03635.pdf
planning chemical syntheses with deep NNs and symbolic AI: https://www.nature.com/articles/nature25978, alphago-inspired architecture
scaling laws for neural language models: https://arxiv.org/pdf/2001.08361.pdf
learning the difference that makes a difference with counterfactually-augmented data: https://arxiv.org/pdf/1909.12434.pdf
linformer: self-attention with linear complexity: https://arxiv.org/pdf/2006.04768.pdf

https://venturebeat.com/2020/07/15/mit-researchers-find-systematic-shortcomings-in-imagenet-data-set/

distillation & amplification:
- alphago zero and the foom debate: https://intelligence.org/2017/10/20/alphago/
- https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446
- https://ai-alignment.com/benign-model-free-rl-4aae8c97e385
- https://ai-alignment.com/policy-amplification-6a70cbee4f34
- "capability vs policy amplification"
- https://intelligence.org/2018/05/19/challenges-to-christianos-capability-amplification-proposal/
- https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616

read more into inverse RL, seems like a very promising way to go about this

https://intelligence.org/all-publications/

ways to meaningfully contribute to ai safety? Doesn't seem there's a way besides actually building the system

read over his papers to know which areas is he working in: http://www.pages.drexel.edu/~ek826/, http://www.pages.drexel.edu/~ad3639/
- https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Modeling_Biological_Immunity_to_Adversarial_Examples_CVPR_2020_paper.pdf
- apparently, he worked on the loihi chip too, and ask about the nitty gritty details on SNNs
- is it easy now to get started on spiking NNs? doesn't seem so
- is there research left for small labs with may be 4 V100
- ask ynes about the actual hardware umass used. Are they using it off-the-shelf?
- funding source???

read over russell's papers: https://people.eecs.berkeley.edu/~russell/research/future/

https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/
https://towardsdatascience.com/language-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054

"expert iteration", thinking fast and slow with DL and tree search: https://arxiv.org/pdf/1705.08439.pdf
neural architecture search with RL: https://arxiv.org/pdf/1611.01578.pdf
neural architecture search: a survey: https://arxiv.org/pdf/1808.05377.pdf

turn faces into anime girls. May be real guy -> anime guy/girl, and real girl -> anime guy/girl as well. Devise a way of switching the network back and fourth, to practice inputting in explicit meaning. This should be the first step in actually extracting information out of a system

may be an extremely dumb question about nano tech. We obviously can move atoms around in a silicon lattice. So why can't we design a self-bootstrapping mechanism that allows us to basically manipulate everything?

can RL from pixels be as efficient as RL from state? https://bair.berkeley.edu/blog/2020/07/19/curl-rad/

https://iclr.cc/

SNN: https://www.youtube.com/watch?v=ICw2_49dSNw

discovering RL algorithms: https://arxiv.org/pdf/2007.08794.pdf

pipeline stuff (the concurrent idea in CPUs and stuff): https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf

squad:
- know what you don't know: unanswerable questions for squad: https://arxiv.org/pdf/1806.03822.pdf
- squad: 100k+ questions for machine comprehension of text: https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf

paragraph stuff, "paragraph recognition:
- http://www.tbluche.com/files/Goog2017.pdf
- scan, attend, and read: end-to-end handwritten paragraph recognition with MDLSTM attention: https://arxiv.org/pdf/1604.03286.pdf

covid stuff:
- https://connect.biorxiv.org/relate/content/181
- continual bert: continual learning for adaptive extractive summarization of covid-19 literature: https://arxiv.org/pdf/2007.03405.pdf
- a survey on applications of AI in fighting against covid-19: https://arxiv.org/pdf/2007.02202.pdf
- IoT for current covid-19 and future pandemics: an exploratory study: https://arxiv.org/pdf/2007.11147.pdf

how classical language models be used in any circumstances without it becoming god? I just sort of don't get it

again with the image segmentation but different color for different objects of same class, "instance image segmentation"

deepmind's DNC (differential neural computer), mentioned by goertzel

one weird trick for parallelizing CNNs: https://arxiv.org/pdf/1404.5997v2.pdf
http://course18.fast.ai/part2.html
dawn bench: an end-to-end DL benchmark and competition: https://dawn.cs.stanford.edu/benchmark/papers/nips17-dawnbench.pdf
why train what you can code? Rekall: a compositional approach to video analysis: https://dawn.cs.stanford.edu/2019/10/09/rekall/
infrastructure for usable ML: the stanford dawn project: https://arxiv.org/pdf/1705.07538.pdf
http://image-net.org/update-sep-17-2019
"resnet inference energy cost"
noscope: optimizing NN queries over video at scale: https://arxiv.org/pdf/1703.02529.pdf
hidden technical debt in ML systems: https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf
E^2-Train: training SOTA CNNs with over 80% energy savings: https://arxiv.org/pdf/1910.13349.pdf
ML: the high interest credit card of technical debt: https://research.google/pubs/pub43146/, https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43146.pdf
"functional vigilance of drivers while using autopilot", lex's MIT research on autopilot

again, why don't ppl include loss surface visualizations in their papers??? I think this is absolutely necessary to actually gain an intuition of what's going on

attention and its relation to event-based systems?

so the idea GPT-3 is pushing is that when the big network gets fed a prompt, it forms this little network on the go that's specialized to handle the task, which sort of explains why GPT-3 needs to be so big, and it also sort of explains the power-law relationship. So really reason about the relative sizes of these networks. How large is the actual specialized network and the full one? Then make comparisons to other animals

also yeah, we already have the A100 GPU, which can do like 600 TFLOPS, more than the human brain already. But the training still takes time tho, because what we're effectively doing is getting the general architecture to pass through billions of years of evolution, so it makes sense that it takes so long and consuming so many resources. But humans learn new things pretty slowly too, which sort of tells us that in learning completely new things, humans take as long as GPT-3. So have numbers to characterize this phenomenon

deepmind, deep RL and its neuroscientific implications: https://deepmind.com/research/publications/Deep-Reinforcement-Learning-and-its-Neuroscientific-Implications
yoshua bengio: from system 1 DL to system 2 DL: https://www.youtube.com/watch?v=T3sxeTgT4qc
seems interesting: https://www.wandb.com/papers
don guy seems interesting: https://www.youtube.com/playlist?list=PLpP2qGSxCw-e0nMetkx41JVrdcivVxUPm, https://www.pugetsystems.com/all_hpc.php
obama interview: https://www.youtube.com/watch?v=Tjl8ka3F6QU

spectral normalization, seems to fix problems with the discriminator

https://www.fast.ai/2018/07/02/adam-weight-decay/

history ppt:
- eliza, futile attempts, example of "symbolic AI". Then the whole expert systems. Mention the brittleness of it
- then checkers, proving out RL and markov chains
- then classic brute force of gary kasparov
- MNIST benchmark, LeNet, Yann LeCun, first important demonstration that the connectionist view actually works. Also proving that convs also works
- ImageNet benchmark, a concrete benchmark, 1k labels, 10M images
- AlexNet, first breakthrough by Alex Krizhevsky & Ilya Sutskever, which blows everyone out of the water. Implementation is so simple too
- ResNet, huge breakthrough in proving deep systems work, and introduce an idea that deeper is indeed better, and don't have diminishing returns. Also impact on loss landscape is amazing
- attention is all you need. Seemingly don't have a ceiling. The bigger you go, the better it gets
- so, ppl have been making it bigger, and consistently get bigger returns. GPT-2, GPT-3, BERT, and results are absolutely amazing
- returning back to RL, there's the alphago stuff. The impact is that now it can teach us instead of deepblue. Also it can be very creative

path from now? Short and long term goals?

long term goal is to be able to:
- show others where is this technology at, what can be done with it, how to think about our future and how to be prepared for it
- understand more about how the brain actually works
- apply image supervised learning in all other domains
- do research on my own with language models
- apply collaborative filtering and dealing with tabular data at scale, and active learning too
- paragraph recognition
- object recognition
- do research on RL
- accurately model physics

short term goal is to:
- read over all papers currently, which is quite a lot
- do all brilliant stuff (by this week)
- do all jeremy howard stuff
- do a few kaggle stuff
- redo scales on the brain capacity thingy, and create a gpu and performance guide for ppl who're into training stuff (actually measure my GPU performance)
- warn teachers of GPT-3, create an interface to automatically generate scripts to amaze them

https://medium.com/dataseries/google-deepminds-dreamer-is-a-reinforcement-learning-agent-that-can-solve-long-horizon-tasks-5faa6f6b63b
https://singularityhub.com/2020/07/26/deepminds-newest-ai-programs-itself-to-make-all-the-right-decisions

really do the image label from minh thingy, to prove out everything I've picked up

also wait, inference energy cost on GPT-3 seems to be quite low (4kWh/100 pages of output). Still 2-3 orders of magnitude away from human brains, but it's actually low enough to be beneficial. So, the bulk energy cost is in the training, where we're essentially doing the evolution thingy again

proximal policy optimization, another popular RL method

intelligence explosion microeconomics: https://intelligence.org/files/IEM.pdf

https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai
https://www.lesswrong.com/posts/Mzrs4MSi58ujBLbBG/you-can-probably-amplify-gpt3-directly

breaking the quadratic attention bottleneck in transformers: https://www.reddit.com/r/MachineLearning/comments/hxvts0/d_breaking_the_quadratic_attention_bottleneck_in/

ready player one

https://www.reddit.com/r/MachineLearning/comments/b179cs/d_the_bitter_lesson/

reddit carreer in researching AGI

again, how to do attention stuff for simple workloads just to test the idea out?

again, think about how to structure an active learning problem to create a fast feedback system. Basic principles should be fairly simple and easy to see, although there need not be actual implementable code to extend from. And yeah, I feel like there is a fear of generating new samples on the go and so ppl are turned away by that and don't necessarily embrace the nonlinearity of the real world

why gpt-3 doesn't use embeddings
language model encodings

https://mailchi.mp/2485e6b42012/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals-for-ai-alignment?e=e50845312e

sparse coding:
- http://www.scholarpedia.org/article/Sparse_coding
- https://blog.metaflow.fr/sparse-coding-a-simple-exploration-152a3c900a7c
- http://ufldl.stanford.edu/tutorial/unsupervised/SparseCoding/
- https://www.cs.ubc.ca/~schmidtm/MLRG/sparseCoding.pdf
- https://www.sciencedirect.com/topics/engineering/sparse-coding
- http://users.ics.aalto.fi/harri/dityo/node3.html
- https://stats.stackexchange.com/questions/118199/what-are-the-differences-between-sparse-coding-and-autoencoder
- https://www.youtube.com/watch?v=7a0_iEruGoM

https://waitbutwhy.com/2018/04/picking-career.html

how to actually do outreach programs

how can RL deal with a cold start

ddpg algorithm

world models: https://arxiv.org/pdf/1803.10122.pdf

https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion
https://www.kaggle.com/twinkle0705/your-starter-notebook-for-osic#Reading-the-data
https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/data

https://blog.gregbrockman.com/how-i-became-a-machine-learning-practitioner
https://www.reddit.com/r/MachineLearning/comments/g16s40/r_metalearning_in_neural_networks_a_survey/

https://towardsdatascience.com/visual-attention-model-in-deep-learning-708813c2912c
https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html

next thursday: https://www.facebook.com/events/297524755014780/
https://dailynous.com/2020/07/30/philosophers-gpt-3/

verify double descent & super convergence with cifar 10
verify the attention stuff in resnet. Try and grab 2 classes from imagenet
https://mml-book.github.io/book/mml-book.pdf
http://www.arxiv-sanity.com/

crafting papers on ML: https://pdfs.semanticscholar.org/3efc/b97c1de1c87832a7a1d99e91801992a938ec.pdf
http://joschu.net/blog/opinionated-guide-ml-research.html
ask reddit how to approach the conversation to ppl about AI, "reddit how to discuss impact of machine learning"

https://www.reddit.com/r/learnmachinelearning/
https://www.reddit.com/r/deeplearning/
https://www.reddit.com/r/MachineLearning/

so temporal-difference is basically just making sure the internal model of reality is good?

AI community's thought on practical ppl like Jeremy?

https://www.cs.cmu.edu/afs/cs/academic/class/15883-f15/slides/

scikit-learn is mentioned a whole lot, so what does it actually offer?

again with the automl stuff

jeremy interview: https://www.youtube.com/watch?v=205j37G1cxw

https://forums.fast.ai/

agi stuff:
- http://www.hutter1.net/ai/introref.htm
- https://www.youtube.com/channel/UCCwJ8AV1zMM4j9FTicGimqA
- https://goertzel.org/agi-curriculum/
- https://cis.temple.edu/~pwang/AGI-Intro.html
- https://en.wikipedia.org/wiki/Artificial_intelligence
- https://www.reddit.com/r/artificial/comments/6cnlr6/monthly_how_to_get_started_with_ai_thread/
- http://www.agi-society.org/membership/
- https://content.sciendo.com/view/journals/jagi/jagi-overview.xml

https://rodneybrooks.com/a-better-lesson/

https://placements.openmined.org/

and yeah, check out all the gui stuff for active learning, cause again, that's hugely important that I can do this very quickly. But goddammit why aren't there a processing-like library for python that can display things directly to the notebook?

http://worrydream.com/MagicInk/
"how much computation the brain does"
- https://www.cs.cmu.edu/afs/cs/academic/class/15883-f15/slides/
- https://www.cs.cmu.edu/afs/cs/academic/class/15883-f15/slides/marr.pdf
- http://www.cs.utexas.edu/~dana/Book1.pdf
- https://www.frontiersin.org/articles/10.3389/fnsys.2016.00095/full

again with the nas & automl thingy:
- https://www.youtube.com/watch?v=a6v92P0EbJc
- https://www.youtube.com/watch?v=tfCA8X4jGjk
- searching for activation functons: https://arxiv.org/pdf/1710.05941.pdf
- https://en.wikipedia.org/wiki/Neural_architecture_search
- an introduction to NAS for convnets: https://arxiv.org/pdf/2005.11074.pdf

meta learning through hebbian plasticity: https://www.youtube.com/watch?v=v2GRWzIhaqQ

"positive strand rna": https://en.wikipedia.org/wiki/Positive-sense_single-stranded_RNA_virus
s1 protein sars-cov-2
"rna based transcriptase": https://en.wikipedia.org/wiki/Reverse_transcriptase

david silver has the idea that goals are distinct on the scale level. On the human level, the goal is to survive, while on the universe level, the goal is to find the fastest way to increase entropy. This interpretation is sort of unique, cause it shows the same idea as the collapse of the quantum function, and that the collapse has to be thought of in the universe playground

https://www.reddit.com/r/MachineLearning/comments/i9kztq/d_hidden_gems_and_underappreciated_resources/

rewatch jeremy again

check over deepmind wiki

http://www.argmin.net/2018/06/25/outsider-rl/
http://karpathy.github.io/2016/05/31/rl/
lex francois round 2

ask about RL training stability to that redditor

jeremy has a new DL course in 2020 so check that out: https://www.youtube.com/watch?v=bHVqO5YyNbU

think about doing an outreach program for AI stuff

cashapp

again with blockchain stuff???? actually implement 1?

try 23andme again, to know what things am I susceptible to

"Google Learning Certificates", https://soha.vn/google-gay-soc-khi-tuyen-bo-ban-khong-can-phai-hoc-dai-hoc-nua-neu-thay-vua-ton-tien-vua-ton-thoi-gian-20200831143938153.htm

again with the venus project? and resource based economy

openai beta as outreach program

may be deep RL introspection in our current systems help us reason about this take off scenario better

https://towardsdatascience.com/bert-why-its-been-revolutionizing-nlp-5d1bcae76a13

https://www.youtube.com/c/ManolisKellis1/videos

on the measure of intelligence: https://arxiv.org/pdf/1911.01547.pdf

https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce
https://towardsdatascience.com/summary-of-tabular-methods-in-reinforcement-learning-39d653e904af

work on simulation demos now:
important:
- electric field
- phased array
- logistic map
- lorenz attractor, 3d
- julia sets

so apparently, carrying 1GW power over 10M volt lines is feasible. So 100 amps, not 10k amps. So that guy may not know about this in depth. So dig more into the numbers and grasp at the fundamentals of these high power systems

also something seems wrong. Why is the hippocampus so small and the cortex so large? Also why the cerebelum is so dense and the cortex so sparse? And what's the energy efficiency (Landauer's limit) of different parts of the brain?

DJ SEO's neural dust: https://arxiv.org/pdf/1307.2196.pdf
neosensory throughput: http://ww-w.eagleman.com/papers/novich_eagleman_2015.pdf

https://www.acrobiosystems.com/P3103-SARS-CoV-2-%28COVID-19%29-S1-protein-His-Tag.html

does catastrophic forgetting occur in RL systems?

pick up an epidemiology book

https://www.technologyreview.com/2020/09/14/1008323/ai-ethics-representation-artificial-intelligence-opinion

logging every request down, and dig about the log rotate thingy

try out wolfram and new kind of science book

seems sort of hopeless to build a computer from reversible components?
- https://en.wikipedia.org/wiki/Reversible_computing
- https://spectrum.ieee.org/computing/hardware/the-future-of-computing-depends-on-making-it-reversible
- https://www.americanscientist.org/article/computers-that-can-run-backwards
- https://www.technologyreview.com/2011/01/25/197408/the-fantastical-promise-of-reversible-computing/

https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model

see options to hoard domains

https://www.reddit.com/r/Entrepreneur/comments/7madgj/ive_been_hoarding_40_domains_its_time_to_sell/
"reddit domain hoarding"

this as reference for the NMRI model: https://deepmind.com/blog/article/neural-scene-representation-and-rendering

https://medium.com/@nainaakash012/efficientnet-rethinking-model-scaling-for-convolutional-neural-networks-92941c5bfb95
domain experts:
- https://www.kaggle.com/wrrosa/advanced-dicom-ct-3d-visualizations-with-vtk
- https://www.kaggle.com/anarthal/dicom-metadata-eda
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/165727
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/166123
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/165253
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/169658
top:
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/169121

https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/164870
dicom image position & reference frames:
- https://stackoverflow.com/questions/14930222/how-to-calculate-space-between-dicom-slices-for-mpr
- https://dicom.innolitics.com/ciods/digital-x-ray-image/x-ray-generation/00180060
- https://www.dicomlibrary.com/dicom/dicom-tags/
- http://dicomlookup.com/
- https://stackoverflow.com/questions/30814720/dicom-and-the-image-position-patient
- http://dicom.nema.org/MEDICAL/DICOM/2014c/output/chtml/part03/sect_C.7.6.2.html#sect_C.7.6.2.1.1
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/174433

fields that don't seem to change at all:
- PixelSpacing - neither (nearly lv 1, outlier in patient ID00099637202206203080121 which changes a heck lot)
- WindowCenter - lv 1. This throws the ragging error. More analysis to follow
- WindowWidth - lv 1. Also throws the ragging error

python pil 3d projections
https://stackoverflow.com/questions/39945068/python-pil-3d-projection, lmao, so no one even cares about this???

seems like no one is talking about the neocortex, aka ppl have no idea how it actually works

reconstruction (think MRIs) seems like a potential ML problem, so really dig into that and try a mock up. If it's possible, then handheld devices that can scan brains can actually work

rocket propulsion elements and ignition again

https://www.reddit.com/r/spacex/comments/d9tkqj/on_regenerative_cooling_raptor_uses_milled_copper/

with 3d stuff: https://www.kaggle.com/gzuidhof/full-preprocessing-tutorial
more brain 3d stuff: https://www.youtube.com/channel/UCDYTPnuWBAVsOVGJ0LTmTgw/featured

https://www.kaggle.com/eladwar/20-seconds-or-less
https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/166957

https://www.youtube.com/c/Brainbook/videos

https://distill.pub/2017/aia/
http://lernapparat.de/

dcm rescale slope and intercept
https://blog.kitware.com/dicom-rescale-intercept-rescale-slope-and-itk/
https://stackoverflow.com/questions/10193971/rescale-slope-and-rescale-intercept

matplotlib 3d
https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html
https://matplotlib.org/3.1.0/gallery/mplot3d/surface3d.html
https://matplotlib.org/3.1.1/gallery/mplot3d/scatter3d.html
https://jakevdp.github.io/PythonDataScienceHandbook/04.12-three-dimensional-plotting.html
https://towardsdatascience.com/an-easy-introduction-to-3d-plotting-with-matplotlib-801561999725

PixelRepresentation

https://www.technologyreview.com/2020/10/16/1010566/ai-machine-learning-with-tiny-data/

"python jupyter gui"
https://stackoverflow.com/questions/53502742/gui-tools-for-jupyter-notebook
https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20Events.html
https://towardsdatascience.com/bring-your-jupyter-notebook-to-life-with-interactive-widgets-bc12e03f0916

another leslie paper: https://arxiv.org/pdf/1506.01186.pdf

https://www.skillshare.com/classes/Motion-Graphics-with-Kurzgesagt-%E2%80%93-Part-1/631970755

https://en.wikipedia.org/wiki/Debiasing

ablation studies in NNs: https://arxiv.org/pdf/1901.08644.pdf
https://stats.stackexchange.com/questions/380040/what-is-an-ablation-study-and-is-there-a-systematic-way-to-perform-it

https://www.fast.ai/2018/04/30/dawnbench-fastai/

"wasserstein distance": GAN: https://arxiv.org/pdf/1701.07875.pdf

"ulmfit vs bert"
https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html
http://jalammar.github.io/illustrated-bert/

create alternate endpoints that make sense, then present it to them

how about creating a lab system to automatically play with chemicals somewhere

https://en.wikipedia.org/wiki/Mechanotransduction

nanoparticle fabrication: https://www.researchgate.net/publication/226347418_Nanoparticle_Fabrication

so do ppl do gene mapping by hand or individual experiments??? Cause that doesn't seem feasible though

alertness
arousal
attention
awareness

look up __all__ again, to stop polluting the namespace

dog whisperer

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216789

"chemistry file formats", "rxn format", "chemdraw file format", "reddit molecular exchange format":
- https://en.wikipedia.org/wiki/Category:Chemical_file_formats
- https://chem.libretexts.org/Courses/Intercollegiate_Courses/Cheminformatics_OLCC_(2019)/2._Representing_Small_Molecules_on_Computers/2.5%3A_Structural_Data_Files
- https://pubs.acs.org/doi/10.1021/ci00007a012
- https://en.wikipedia.org/wiki/Chemical_table_file
- https://en.wikipedia.org/wiki/CDXML
- https://en.wikipedia.org/wiki/CDX_Format
- https://en.wikipedia.org/wiki/ChemDraw
- https://www.reddit.com/r/askscience/comments/accnx4/what_is_the_difference_between_timedependent/

organic chem, "steric hindrance":
- https://www.masterorganicchemistry.com/2012/06/05/nucleophiles-and-electrophiles/
- https://www.chemistrysteps.com/sn1-sn2-e1-e2-choose-which-mechanism/

docker permissions:
- https://stackoverflow.com/questions/26500270/understanding-user-file-ownership-in-docker-how-to-avoid-changing-permissions-o
- https://docs.docker.com/engine/install/linux-postinstall/
- https://vsupalov.com/docker-shared-permissions/
- https://blog.ippon.tech/docker-and-permission-management/
- https://medium.com/@nielssj/docker-volumes-and-file-system-permissions-772c1aee23ca

https://read.amazon.com/?asin=B014TMUFX6
https://en.wikipedia.org/wiki/Epigenetics
https://github.com/fastai/course-v3/blob/master/nbs/swift/01_matmul.ipynb
https://github.com/fastai/course-v3/tree/master/nbs/dl2
https://github.com/tensorflow/swift/blob/master/docs/DesignOverview.md
https://www.fast.ai/2019/01/10/swift-numerics/
https://en.wikipedia.org/wiki/Value_type_and_reference_type

https://github.com/gordicaleksa/pytorch-original-transformer

https://syncedreview.com/2020/11/06/deepmind-proposes-graph-theoretic-investigation-of-the-multiplayer-games-landscape

simulate if smell gradients can actually be detected to know if the complement system actually works

cortical columns: https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00186/full

reticular activating system
galanin
arcuate fasciculus

dealing with smiles:
- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6764162/
- smiles self attention: https://pubs.acs.org/doi/10.1021/acs.jcim.8b00803
- CNN for detecting chemical motifs

rewatch tesla and neuralink ppts

- integrative analysis of 111 reference human epigenomics: https://www.nature.com/articles/nature14248
- FTO obesity: https://www.nejm.org/doi/full/10.1056/NEJMoa1502214

https://www.reddit.com/r/science/comments/4pmivr/science_ama_series_im_manolis_kellis_a_professor/
irx3 and irx5 genes: https://en.wikipedia.org/wiki/IRX3, https://en.wikipedia.org/wiki/FTO_gene
prdm9: https://en.wikipedia.org/wiki/PRDM9

cortex layer 5
5ht 2a/2c receptors: https://en.wikipedia.org/wiki/5-HT2A_receptor

nmn, cd38 protein, mtor pathway, foxo3 gene, ccr5 gene, doxycycline, exosome injection, prp injection, yamanaka reprogramming factors, aav virus

so seems like, you can plug everything into pcie and it will just magically work?

https://github.blog/2020-11-20-nbdev-a-literate-programming-environment-that-democratizes-software-engineering-best-practices/

https://www.researchgate.net/publication/317841703_The_oldest_fossil_mushroom
https://www.researchgate.net/publication/343188979_Fungi_in_the_rear_mirror_A_brief_history_of_the_fungi_during_the_last_two_billion_years
https://en.wikipedia.org/wiki/Pseudomonas_aeruginosa

pfizer vaccine paper: https://www.medrxiv.org/content/10.1101/2020.06.30.20142570v1

https://news.ycombinator.com/item?id=17392461
https://www.reddit.com/r/RISCV/comments/gme72d/hifive1_revb_blinking_led_on_a_breadboard_using/
https://www.reddit.com/r/RISCV/hot/

how drug design is done
astrazeneca
transcranial magnetic stimulation
fusiform face area
middle temporal visual area (V5)

16 psyche
making sense of pharmacology: inverse agonism and functional selectivity, https://academic.oup.com/ijnp/article/21/10/962/5066769
drug design and discovery: principles and applications: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6155886/
could a neuroscientist understand a microprocessor: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268
https://www.sciencemag.org/news/2020/11/game-has-changed-ai-triumphs-solving-protein-structures
https://www.nih.gov/coronavirus
https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost
https://www.nature.com/scitable/topicpage/the-role-of-methylation-in-gene-expression-1070/
https://docs.spring.io/spring-framework/docs/4.0.x/spring-framework-reference/html/beans.html
do the pid loop simulation: https://ksp-kos.github.io/KOS/tutorials/pidloops.html, https://en.wikipedia.org/wiki/PID_controller
architecture of sars-cov-2 transcriptome: https://www.sciencedirect.com/science/article/pii/S0092867420304062
d614g mutation: https://www.nature.com/articles/s41586-020-2895-3
entire covid 19's genome: https://www.ncbi.nlm.nih.gov/gene/43740568

https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5641975/
https://pubmed.ncbi.nlm.nih.gov/25561484/

curious stuff:
- https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/
- https://vimeo.com/238243932
- https://www.ijcai.org/Proceedings/2017/0344.pdf
- https://arxiv.org/pdf/1606.01868.pdf
- https://www.youtube.com/watch?v=0yI2wJ6F8r0
- rainbow: https://arxiv.org/pdf/1710.02298v1.pdf

also make videos on youtube about my collection of kerboscripts

other interesting notes:
- a lot of times the network is stuck because oscillation frequency is too high. What is observed is the actions' oscillations seem to coincide with the locations' oscillations, but somehow the phase is just off and leave the system oscillating forever
- can it be that the fourier thingy sort of hides a prior behind it to make it oscillate a lot. To test this out, try this with other environments

also ksp automated flight and landing

post my stuff on kos subreddit

try to use manim

https://www.chemistrysteps.com/sn1-sn2-e1-e2-choose-which-mechanism/

that combined knowledge thing:
https://deepmind.com/blog/article/neural-scene-representation-and-rendering
https://science.sciencemag.org/content/sci/360/6394/1204.full.pdf?ijkey=kpkRRXA1ckHD6&keytype=ref&siteid=sci

https://www.researchgate.net/post/Can_anyone_suggest_to_me_software_to_view_gene_annotations
https://en.wikipedia.org/wiki/Homeobox

citric acid cycle again

simulate synthetic aperture radar, start from scott manley's vid

simulate a simple pid, with initial state at 0 and new state at 1. Still worse than the intended thing, but just to get this out quickly

https://en.wikipedia.org/wiki/Schlenk_line

genome enrichment, epigenomic enrichment, gene set enrichment analysis
omics
neuronal energetics: https://www.sciencedirect.com/science/article/pii/S0960982214002723
rs1421085

how to see crystal structures within a metal?

also good bg music for videos?

tesla's quarterly reports?

also figure out what is the current staining capability? Cause that's what I expect how engineering proteins can be useful

https://decatopus.com/project/thouless-pump-simulation/
https://decatopus.com/wp-content/uploads/application/pdf/Thesis.pdf

full genome sequencing price, https://www.cnbc.com/2019/07/01/for-600-veritas-genetics-sequences-6point4-billion-letters-of-your-dna.html

mass production: https://www.reddit.com/r/Entrepreneur/comments/9n8xyj/inventrepreneurship_i_took_an_idea_to_mass/

borg warner, cost analysis

https://en.wikipedia.org/wiki/Stainless_steel
https://en.wikipedia.org/wiki/Carbon_nanotube

also do a youtube series on the RL stuff so far

oncogenes: braf, her2, bcr-abl

difference between senescent cells and cancer cells?
https://en.wikipedia.org/wiki/Proteostasis
https://en.wikipedia.org/wiki/Cloning
https://en.wikipedia.org/wiki/List_of_animals_that_have_been_cloned
melanoma brain metastases
target of rapamycin, TOR and mTOR
metabolic control enzyme known as AMPK
extrachromosomal ribosomal dna circles, or ERCs
tamoxifen activates I-PpoI gene? How does that work?
also apparently, that protein in milk, casein, does not break down fast while in the body. This is kinda strange, so look into it, https://en.wikipedia.org/wiki/Protease
https://en.wikipedia.org/wiki/Archaea
https://en.wikipedia.org/wiki/Southern_blot
radioactive dna probe, hybridization probe
also look up how insulin is made, to understand how recombinant dna works

https://www.ncbi.nlm.nih.gov/nuccore/NR_034146.1
https://www.ncbi.nlm.nih.gov/genome/gdv/browser/gene/?id=22933

temp discarded:
- https://www.kaggle.com/jhoward/don-t-see-like-a-radiologist-fastai
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/165727
- https://www.kaggle.com/nxrprime/fibrosis-eda-fast-ai
- https://www.kaggle.com/dipam7/intel-scene-progressive-image-resizing

how can I know chemical concentrations of my blood to track it pretty much daily? Does such a device even exist?

interactome
zinc fingers sound super useful tho, but manolis said it's a pain in the ass to deal with
play fold it again? just to see what the problem actually is
https://en.wikipedia.org/wiki/Structural_alignment

signal strength stuff

apparently they machine manufacture machines right in house? Isn't that super complicated though? How did they do it? Talk to someone who has designed such a thing before

2005 paper on AC2 receptor: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1287568/

yeah so I've heard that antiviral drugs sort of uses small molecules to change the shape of some proteins, like S1. Have anyone sort of design proteins so that they can directly bind to them?

why can't I flood the body with AC2 receptors with some pieces of cell membrane attached to it, to simulate an actual cell, so that all the covid 19 viruses sort of binds there and therefore can't bind to the AC2s in actual cells?

buy those bio hacking kits?

ACE2, much more than just a receptor for sars-cov-2: https://www.frontiersin.org/articles/10.3389/fcimb.2020.00317/full
mRNA closed loop model: https://www.cell.com/molecular-cell/fulltext/S1097-2765(18)30941-9
anti-ACE2 antibodies??? https://www.rndsystems.com/resources/articles/ace-2-sars-receptor-identified

antenna gain
https://joininteract.com/fellowship/

how to actually control a stepper motor??? This is the high power electronics I was talking about

buy a cheap iphone to test things out, and to know the infrastructure well enough

and how to do the packaging stuff and sending stuff to usps???

etsy and https://www.easyship.com/blog/ecommerce-sites-for-wholesalers

https://en.wikipedia.org/wiki/Gas_laser
seems like high power lasers can be done using: https://en.wikipedia.org/wiki/Q-switching

may be pick up a little chinese

okay, so everything seems to be going well so far... Still have that problem of controlling high power electronics with low power digital systems like arduinos ("arduino with mains voltage controller") though, so look more into that. And where the hell can I buy gears????

also how are diamonds made industrially? Is it just like growing carbon nanotubes?? And find some pictures of its molecular structure and compare that to the natural diamonds. May be helpful: https://s3-us-west-2.amazonaws.com/prd-wret/assets/palladium/production/mineral-pubs/diamond/diamon01.pdf, https://www.diamonds.pro/education/lab-created-diamonds-prices-value/

ttt diagrams: https://www.tf.uni-kiel.de/matwis/amat/iss/kap_8/illustr/s8_4_3a.html

also calculate stress structures within a home, and see what material properties do the concrete blocks need

https://www.omc-stepperonline.com/nema-17-stepper-motor/, with https://en.wikipedia.org/wiki/Digital_signal_controller, "stepper driver controllers", https://www.orientalmotor.com/stepper-motors/index.html

for the high power thingy, see over 24V power supplies pcb designs, to get a sense of what things are on there

also, what's the curing process involving an autoclave looks like for carbon fiber?

and finally, analyze tesla's costs to make sense of everything

gene therapy again

reddit thread about ACE2 receptors

kvm resume

update story of energy

https://en.wikipedia.org/wiki/DNA_nanotechnology
https://en.wikipedia.org/wiki/Molecular_self-assembly
https://en.wikipedia.org/wiki/Non-contact_atomic_force_microscopy

https://en.wikipedia.org/wiki/ELISA
https://www.thermofisher.com/us/en/home/life-science/protein-biology/protein-biology-learning-center/protein-biology-resource-library/pierce-protein-methods/overview-elisa.html

goals for this trip?
- take over truckbux operations
- grow stocks
- have some skills in building simple products, like just laser cutting stuff, or metal injection molding parts, starship-related
- machine parts for complicated machine. Goal should be to machine dies or complicated parts for precision instruments
- get a good feeling of how things work in AI, especially how to debug stuff, and really analyze stuff from the inside. Pretty much what I'm doing right now, but just do it over and over again
- actually do some genetics, and see if there's actually a real chance to do this or not
- demonstrate plasmid cloning. Heard it's super simple and whatnot
- demonstrate PCR, to get in touch with the querying tools
- understand the compliment system designs, and see if I can come up with comparable systems
- create a system that accurately maps out distances of a camera feed when also given the coordinate system
- do a bunch of comparative genomics. Basically figure out the story of every life on earth. Also try to run AlphaFold2. Also try to figure out the energy landscape in all of this. Targets:
	- DNA polymerase
	- cap binding complexes
	- ribosome subunits
	- pseudouridine synthase: Z71568, YNL292w, https://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi?seqinput=18158779
	- mitochondria, atp synthase: NC_012920, 251831106
	- methane monooxygenase: CH4 + O2 + NAD(P)H + H+ -> CH3OH + NAD(P)+ + H2O, single bond replacements
	- amylase, alcohol dehydrogenase, hemoglobin, fibrin, collagen, actin, insulin, potassium channels, luciferase, hemagglutinin, HIV-1 protease, p53 tumor suppressor, cellulases
- try to infect cells with modified virus with a specific payload
- actually try to run alphafold: https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13
- intercept seek thermal communications
- try to build robot arms that can do things quickly, and have a good internal model of reality
- another posibility for sequencing techniques is to have a sort of ribosome that enlarges the base pairs, so we can see them exactly with a regular microscope
- apparently, psilocybe cubensis exists in vn, so go look for them in actual forests, or 
- figure out all reactions in seablock
- attempt to make a kiln

https://www.chems.msu.edu/resources/tutorials/ASPEN
https://en.wikipedia.org/wiki/Process_manufacturing
https://en.wikipedia.org/wiki/Aspen_Technology

https://en.wikipedia.org/wiki/Contig
https://en.wikipedia.org/wiki/Scaffolding_(bioinformatics)

recalculate fundamental energy costs again, this time with a focus of training RL agents for molecular bio's degrees of freedom. Figure out if Tesla's 10TWh/yr throughput is enough

Transcriptome Shotgun Assembly, context: https://www.ncbi.nlm.nih.gov/genbank/release/240/

biowin

covid genome: https://www.ncbi.nlm.nih.gov/nuccore/NC_045512
covid characteristics: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7402395/
aryl: https://www.ncbi.nlm.nih.gov/protein/NP_001612.1

https://hostdefense.com/
https://fungi.com/

YP_009724389.1

https://en.wikipedia.org/wiki/Proteolysis
https://en.wikipedia.org/wiki/Signal_peptide
https://en.wikipedia.org/wiki/Secretion
https://en.wikipedia.org/wiki/Protein_targeting#Protein_translocation
https://en.wikipedia.org/wiki/Post-translational_modification
https://en.wikipedia.org/wiki/Cytosol
https://en.wikipedia.org/wiki/Glycosylation
https://en.wikipedia.org/wiki/Psilocybe
refseq: https://www.ncbi.nlm.nih.gov/books/NBK21091/
https://en.wikipedia.org/wiki/Plasmid
https://en.wikipedia.org/wiki/Baltimore_classification
https://en.wikipedia.org/wiki/Molecular_machine
https://en.wikipedia.org/wiki/Human_mitochondrial_genetics
https://en.wikipedia.org/wiki/ATP_synthase
https://en.wikipedia.org/wiki/Ribosomal_frameshift
https://en.wikipedia.org/wiki/Beta_sheet
https://en.wikipedia.org/wiki/TIM_barrel
https://en.wikipedia.org/wiki/Physarum_polycephalum

question: can everything be built using proteins alone? Like, can stuff like cell membrane (lipid bilayer) be produced just by using proteins? If they can, then what's the exact mechanism

https://codexdna.com/products/bioxp-system/

comprehensive source: https://berthub.eu/articles/posts/reverse-engineering-source-code-of-the-biontech-pfizer-vaccine/

again, what's up with bacteriophages???

also, the recording stuff is really, really crude. Like, simple pink colors for acid/bases??? That's it? That's like 1 bit of information dude. So how about, bind rare atoms to specific complexes, and have those attaches to any structure you want. Then we can excite those rare atoms, and record their EM spectrum, for a lot of data bits. Also apparently florescent systems don't have rare atoms, so figure out how the hell can they do that??? Edit: specific proteins to find: luciferase

also, what's up with the polyprotein nature of covid? Actually you know what, I heard many proteins are like that too, like histones, or hemoglobin, or atp synthase, so figure out the general dynamics of these systems

also apparently, some staining methods like gram pos/neg ppl have no idea how they work. And methods to visualize brains too, with silver atoms or sth like that

https://fold.it/portal/node/2010076
https://fold.it/portal/node/2007798
https://www.reddit.com/r/MachineLearning/comments/k4n3m2/d_deepminds_alphafold_2_explained_ai_breakthrough/

so far, this seems really, really hard. But giving up is for pussies, so play foldit for a long time to roughly understand how stuff works in this domain

https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/

do factorio bad apple track

this company offers a virtual tour, so check it out: https://www.sageglass.com/en/article/what-electrochromic-glass

probably not needed, but calculate cost of hydrogen electrolysis as fuel, and compare that to just ordering methane

what is transparent in infrared

https://www.youtube.com/c/Thesignalpath/videos

band gap transparency
https://www.quora.com/What-makes-an-object-transparent
https://physics.stackexchange.com/questions/46673/transparency-of-solids-using-bandgaps-and-relation-to-conduction-and-valence-ban
https://www.quora.com/What-is-opaque-to-visible-light-but-transparent-to-infrared-light
https://physics.stackexchange.com/questions/121696/why-does-glass-absorb-infrared-light
https://physics.stackexchange.com/questions/267793/why-is-glass-transparent-to-visible-light-and-opaque-to-ultraviolet-amd-infrared

revamp seablock, so that it's realistic

hostile takeover?

https://en.wikipedia.org/wiki/Cracking_(chemistry)

also apparently there's this thing called catalytic reforming, that can shape small hydrocarbon chains into aromatic hydrocarbons, like benzene

also why the hell are there so many polymers? like pvc, polystyrene, and whatnot. See advantages and disadvantages of each of them, and see if I can replicate a small sample: PMMA, PS, PE and PP, PETE, ABS, Nylon, PMMA (acrylic), PC (polycarbonate)

what's up with otter, the company?

go over 23andme analysis

may be the only reason ppl can't automate stuff like plaque assays is because they sort of require lots of different instruments that are bulky and expensive. But why are they so freaking expensive? Like, what's the actual cost of assembling 1 for myself that meets international standards???

crystal violet seems to be a really popular staining method. It's also used in gram's method and whatnot

https://en.wikipedia.org/wiki/Virus_quantification, pfu/ml
https://en.wikipedia.org/wiki/Fluid_catalytic_cracking

https://www.biorxiv.org/content/10.1101/2020.10.31.362848v2

subgenual cingulate cortex

old aging stuff:
- https://sites.google.com/view/sources-aging-part-ii/startseite
- https://journals.physiology.org/doi/full/10.1152/ajpheart.00287.2018?rfr_dat=cr_pub++0pubmed&url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&
- https://sci-hub.scihubtw.tw/10.1007/s10522-006-9043-9

vmd atom selection language ("vmd not all protein"): https://www.ks.uiuc.edu/Research/vmd/vmd-1.3/ug/node132.html

high-speed synchrotron x-ray imaging

also, what's the diamond, graphene and carbon nanotubes making process? All 3 of them seems to be in very different configurations, so the heating solution doesn't seem to be that effective

https://en.wikipedia.org/wiki/Quantum_dot

dig again at clearinghouses

world models: https://arxiv.org/pdf/1803.10122.pdf

do cifar 10 again, to get any insights possible, then my custom labels

also try again the style transfer, to see how my architecture can deal with such a dynamic environment

nixie tubes: https://www.youtube.com/watch?v=wxL4ElboiuA

how the spike protein works: https://www.nature.com/articles/s41594-020-0479-4

buy a compressed air can

I don't understand the locking mechanism and chirality that much. Thought experiment seems to invalidate that. So look up for cis and trans 1-4 polyisoprene (natural rubber)

calculate fuel usage of blast furnace in seablock and whatnot. This also means that I should translate all values to real life production rate values

actually try to implement the attention idea that creates consciousness

dive more into crystal structures of steel, cause it seems like martensite has a lot more than just "great internal stress". Look into nitinol (nickel titanium) too. And titanium production

silicon nitride's ceramic properties?
titanium production process? Heard from elon that it's quite complicated

build some exotic shaped metal, following generative design, and see if stresses hold up. Then deduce if doing it that way is worth it or not

rapidly iterating on alloy composition sounds pretty hard tho, so find ppl who are actually working on that

also, how can you sort of figure out what the reactants are going to be? Like, just record a bunch of properties, and devise a bunch of hypothesis?

also what's the cost of galvanized steel vs stainless steel? Like, stainless steel seems much more straightforward, and lasts a long time, so why does galvanized steel even exists?

what actually stops a regular person from actually building an oil refinery? Like, it's just a distillation column that you have to manage pressures and whatnot right? So what's up?

also see how advanced stuff-pickup programs are, and can they do stuff really freeflow?

mri scanners apparently uses liquid helium as a coolant for its superconducting magnets. So what's up?

alternative steel making process, for a glimpse of what's the direction now: https://www.sciencedirect.com/science/article/pii/S0959652618326301

actually attempt to build a computer from scratch in factorio, so as to understand how the linux kernel works, and how computers work in general. May be just play with older hardware, as there wasn't any paging stuff going on

invest in rocket lab

https://www.mt.com/vn/vi/home/products/Laboratory_Analytics_Browse/pH-meter/sensor/ion-selective-electrode.html

uniprot

https://8020.net/

novozyme, research lab that produces industrial enzymes, microorganisms & biopharmaceutical ingredients. Not billion dollars yet, just on the million dollar scale

how to actually observe protein mechanics as it's happening?

https://en.wikipedia.org/wiki/Pattern_recognition_receptor

epimap

transcription seems really energy intensive. Like, it has to break a ton of those hydrogen bonds. So how can we actually calculate the energy of the breakage, and how much energy is actually consumed by the ribosome?

those microarrays? episomal reporter assay? massively parallel reporter assays? also just common plasmid tech used to make a bunch of proteins? microfluidics?

also how to ppl do genetics in crime detective stuff?

chaperonin, huge protein complex where the folding takes place in. Sort of like a shield from interactions from the outside world

how the hell do biotinylation works??? Like, where does the protein-specific selection happen? streptavidin too, cause this is the labelling mechanism

https://en.wikipedia.org/wiki/Genome-wide_association_study

okay I get how zinc fingers can sort of be used to replace dna segments. So is it that hard to design them? Look for stories from ppl who actually designed them

type 2 restriction enzyme = halk folk endonuclease

also direction and stranded-ness of everything??

discovery path for proteins? Where are the entropy leaks?

https://en.wikipedia.org/wiki/Transcription_factor
https://en.wikipedia.org/wiki/Transfection
https://en.wikipedia.org/wiki/Mutagenesis
https://en.wikipedia.org/wiki/Crosslinking_of_DNA
zinc finger vs crispr

titanium: $4.5k/tonne
aluminium: $1.5k/tonne, 2.7kg/L, 10000W/kg?
iron: $85/tonne, 8kg/L

why energy from carbon is cheaper than from electrolysis? Considering renewables are dowxn by quite a lot? And is aluminum carbide really a lot more stable than CO2? Cause CO2 seems hella stable

kroll process

what's up with system V? How does that compare with init? By extension, dig into /etc/inittab

analyze python's grammar file, and python's integration with other things, like C++ for numpy. And try to build it from source. Try to build pytorch from source too

binding languages together: https://emscripten.org/docs/porting/connecting_cpp_and_javascript/Interacting-with-code.html#interacting-with-code-binding-cpp

try to create an agent that has some sense of higher dimensions

basic energy computer analysis again. flip flop high resistor value: 1/(1.4*C*f), capacitor consumption: CV^2 joules/cycle. Not sure about individual computations tho, and not sure how this's supposed to work in GPUs

http://www.aosabook.org/en/index.html

https://caseyhandmer.wordpress.com/2020/02/15/dont-stage-off-starship/
https://caseyhandmer.wordpress.com/2019/11/02/starlink-is-a-very-big-deal/

I really should include a projects section in the resume if space is a lot

https://learning.edx.org/course/course-v1:LinuxFoundationX+LFD110x+1T2021/home
https://riscv.org/community/learn/

jim keller said that if you implement matrix multiply willy nilly using cuda and whatnot, you'll get 10% GPU utilization. Actually try to do this. If successful, then try to find out if I can rebuild the PyTorch stack for swift right from the fundamentals

again with jim keller. He said that the CPU outrageously run things out of order, and do things ahead of time, even after a branching instruction. But I thought every detail on how memory is loaded into registers are contained within the assembly code itself already? How the hell can you change execution flow without also changing what the assembly code does?

add llvm to resume lmao
also add JOGL
also xilinx vivado

how jupyter works underneath again?

see a little bit over cudnn, and write a series of blog posts about how it works underneath, and show concrete numbers and visualizations to talk about

I've heard pytorch has a built-in JIT, so what's up with that? JIT for what? Cause CUDA kernel calls don't really need that rite?

carnot efficiency of the universe?

stuff I really shouldn't put in nodes cause I want to do them so bad:
- https://people.cs.umass.edu/~pthomas/courses/CMPSCI_687_Fall2020/687_F20.pdf
- https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html
- file:///home/kelvin/Downloads/David%20A%20Sinclair%20-%20David%20A%20Sinclair%20Lifespan%20Why%20We%20Age%20and%20Why%20We%20Dont%20Have%20To%20with%20Matthew%20D%20LaPlante%202019%20ISBN%20978-1501191978%20(2019).pdf
- https://www.ncbi.nlm.nih.gov/tools/gbench/tutorial1/

orf3a, tiny gene encoded within the other gene
ridiculous evolution: s1, orf8, orf6

pcr measuring virus particles, actually I just want to understand the testing method for covid

https://www.technologyreview.com/2019/08/02/131198/china-squirrel-has-started-a-grand-experiment-in-ai-education-it-could-reshape-how-the/

event-driven knowledge seeking seems to be much, much more efficient than just a clean sweep. But you kinda have to seek out turbulence and ready to bite the bullet. See if this fundamental behavior/preference is replicated in RL and whatnot. Also again, have an intuitive understanding of the limits of the networks, and where the bottlenecks are

radar vs ultrasonic? Why both on a tesla?

https://en.wikipedia.org/wiki/Induced_pluripotent_stem_cell
https://en.wikipedia.org/wiki/Reprogramming
https://en.wikipedia.org/wiki/Mitochondrial_replacement_therapy

gbench marge views?

supercomputer/cluster stuff that I don't really understand:
- "mpi rank"
- https://stackoverflow.com/questions/5399110/what-is-the-difference-between-ranks-and-processes-in-mpi
- https://www.codingame.com/playgrounds/349/introduction-to-mpi/mpi_comm_world-size-and-ranks
- https://www.open-mpi.org/doc/v4.0/man3/MPI_Comm_rank.3.php
- https://wgropp.cs.illinois.edu/courses/cs598-s15/lectures/lecture22.pdf

how the hell can heme be a protein?

damn, quick visualization tools online for specific proteins kinda sucks

so, all the stuff in biology has to sort of be resilient to tiny changes in the DNA. Like, at least something like 70-90% of mutations (SNV) won't affect how it works at all (<0.001% for engineered systems. Like, if you change 1 bit about code, the program won't compile at all). But there has to also be a massively parallel compute engine to actually make it work. Luckily there are literally trillions and more quantum computers just doing all the data crunching called organisms/viruses. So analyze the compute scale we're talking about here?

why pfizer needs to be really cold? I mean, I've heard that normally, specific proteins within the cells break mrna down. Dna is ridiculously strong (single strand can survive in boiling water just fine), so they don't break at all. So may be just don't include in the proteins that break them down? May be it's hard to separate proteins? But like seriously, that problem seems really lame that we should be able to do it by now

also how does the intelligence cells (macrophage) actually determines who's us and who's the enemy? Like, how does that association even happen? Cause like, this is already a challenge in RL systems

also check claim that coding regions have more GC-content, and longer ones have even more

how pcr tests work again? And why are they so god damn expensive?

how to know resonance frequency of a static structure? Are there general guidelines?

see how vmd interfaces with the embedded tcl interpreter, so that I can hopefully do the same one day with python (https://docs.python.org/3/extending/embedding.html)

so, let's say I have a gene file, then a transcription of that, and a translation of that as separate files. How to I merge them together? SOD1_datasets file seems like it will be impossible, cause like, there are no spacing information inside of the mrna and protein files

actually, shader pipeline seems approachable now, so dig into it at a later time. Both processing and vmd has those shader programs, and they really don't look that intimidating

https://en.wikipedia.org/wiki/Structural_genomics
https://en.wikipedia.org/wiki/Structural_biology

vmd-1.9.4a51/proteins folder, also see over older versions changelogs. Those should be interesting. gbench's: https://www.ncbi.nlm.nih.gov/tools/gbench/releasenotes/

ncbi also has some kind of selection language, so see over that: 21[Chr] AND "Homo sapiens"[Organism] AND ("has ccds"[Properties] AND alive[prop]) 

also why the hell are there so many superoxide dismutase? Shouldn't 1 be enough tho? And again, they appear all over the place

again, how do inverters work? And like, it seems like I have to try to match the grid's frequency and phase, or else it might blow it up. If basic electronics are worked out to a reasonable level of confidence, I can consume as much energy as I wanted and not feel guilty lol

actually work out the cost to lay out landlines across a country, cause like, rural parts of VN still seems to have internet connection just fine

see over NC sequences, and see if they have gap information or not

see how to do a local blast

seems like a nice primer: https://blast.ncbi.nlm.nih.gov/Blast.cgi

try to align 2 serparate superoxide dismutase, in regular python

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3018811/

try pyanodons

so I heard that the payments settlement takes place later than the agreement, so why the hell is that? And why are there layered structures in the economic model?

alright, so coffee stuff
half life: 3-7 hours, decrease 30-50% in male smokers
pregnant woman: <200mg/day = 2 cups instant coffee = 0.5-2 cups fresh coffee
normal ppl: up to 400mg appears to be okay
ld50: 150mg/kg

while doing pyanodon, try to pay attention to a few fundamental questions:
- how to balance figuring out the rules of the game, the time taken, and throughput/effectiveness produced? Right now, the general way is to at least have a grasp of what things are like, and see their natural language description, and match that with reality, to see whether it's actually an important process in a lot of things. Hard start situations in AI systems seems a lot harder tho. Once basic mechanism is worked out, see if I can get rid of side effects, as much as possible (multiple elements in, 1 element out). If side effects (multiple elements out possible), then figure out at least 2 mechanism with those 2 side effects, but one is more and the other is less, so as to have a speed dial on everything. And try to figure out loops for the side effect

attention mechanism. Great that the basic system is working, but what is the fundamental principles? What interfaces are available, what RL mindset would be sufficient? How to link different attention concepts?

https://spacexfleet.com/

check cpu over clocking config in bios

cpu clock sometimes increases to 4.7GHz boost really quickly. But CPU usage in htop is still really low. So how is utilization measured? Does the CPU like, go to sleep that often? Also sometimes it even bossts to 5GHz???? How?

what does it take to build a custom fab lab?

actually capture a bit flip in memory, to measure the rate. Others reported 1 error/(h * GB). memtest86 already did this?

seems like everywhere I go, the concept of JIT compilation crops up all the time. And the compiler infrastructure where layers of compilers perform translation between different domains, just because separating a task into multiple domains can help accelerate the technology. This is how you beat the S curve and make it into an exponential curve. The domains has to sort of be completely separate, and communicates with each other using a simple interface only, and domains spaced apart by another domain should not know each other at all

can't you like, heat treat 3d printed metal parts?

backend fundamentals, add a note by saying that a lot of stuff don't really make sense, so just suggest ideas right away to improve code quality and whatnot

so seems like, there's a hard limit on performance for each process node. So, intel's 'innovation' at 14nm is really just trying to squeeze out performance to get to this ideal? See if the actual performance numbers reflect that

so, deconstructing vendor app?
- read through the code to roughly knows what it's like
- identify interesting infrastructure components
- try to do a vanilla app and test it on my phone/emulator
- try to actually run the vendor app on my phone/emulator
meanwhile, deconstruct as much blackboxes as possible

also how updates work?

https://www.hexnode.com/mobile-device-management/help/how-to-allow-auto-update-of-selected-apps-from-google-play-store/
https://developer.android.com/guide/playcore/in-app-updates
https://support.google.com/googleplay/answer/113412?hl=en
https://www.hexnode.com/mobile-device-management/help/how-to-allow-auto-update-of-selected-apps-from-google-play-store/
https://stackoverflow.com/questions/34251575/how-to-autoupdate-android-app-without-playstore-like-facebook-app-or-any-contes
https://support.google.com/googleplay/android-developer/answer/10355942

https://bogdan-lyashenko.github.io/Under-the-hood-ReactJS/

https://developer.android.com/guide
