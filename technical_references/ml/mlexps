
like 18-dataset-size, but this time sort of randomizes y values a lot, so that there're many samples on the right and a few on the left, to be more consistent

task: image-classification/
- cifar10:
	- persistent depth, squeeze in width of single layers from 30 and see results
	- batch norm, conv, activation order
- same thing with random dataset from imagenet, but higher resolution, and far/near (dog vs squid, and species of dog) examples
task: style transfer
- see if modifying images over time have different internal dynamics than normal training
task: char name classification
- attention approach (already did that), same old scale experimentation
- bigru approach, understand underlying C++ codebase
task: progressive deepening
- high-res gan stuff. This should verify learning dynamics at later times (think: blind cat)
- grok again, because I didn't use adamw

some metas tag, to be more SEO friendly and have a nice preview

optics simulation?

cripple function network early (zeros out all forward gradients after a specific middle layer), see if it becomes steady state or not. My prediction would be it'd drive changes to zero
