
Decoupled teams:

Application Ops/SRE
        ^
        |
     K8S API
        |
        v
Cluster Ops/SRE
       ^
       |
  Kernel SysCall API
       |
       v
Kernel Ops/SRE
        ^
        |
  CPU (x86, ARM, etc.)
        |
        v
Hardware Ops/SRE

Ops = Operations teams
SRE = Site Reliability Engineers

Application programs, typically:
- language runtime
- libraries
- source code
- external shared libraries, libc, libssl

Google Cloud Platform registry, "Google Container Registry"

Generally in K8S, containers are launched by a daemon on each node called the kubelet

Google: https://cloud.google.com/kubernetes-engine/
Setting default zone: gcloud config set compute/zone us-west1-a
Create a cluster: gcloud container clusters create kuar-cluster. Takes several minutes
Get cluster credentials: gcloud auth application-default login

Azure (aks = Azure Kubernetes Service):
Creating resource group: az group create --name=kuar --location=westus
Create a cluster: az aks create --resource-group=kuar --name=kuar-cluster. Takes several minutes
Get cluster credentials: az aks get-credentials --resource-group=kuar --name=kuar-cluster
kubectl tool: az aks install-cli

AWS (eks = elastic kubernetes services):
eksctl create cluster --name kuar-cluster
eksctl create cluster --help

Locally, minikube:
minikube start
minikube stop
minikube delete

lots of docs:
- https://kubernetes.io/docs/home/
- https://kubernetes.io/docs/reference/kubectl/overview/

kubectl version:
- local kubectl tool version
- k8s api server version

kubectl get componentstatuses: cluster diagnostics

controller-manager: entry.cmd
scheduler: worker.cmd
etcd: storage for cluster, where all API objects are stored

cluster components, components that make up the cluster, actually deployed using kubernetes itself. These have sort of like important concepts to fathom. All of these components will run in the kube-system namespace.

Kubernetes Proxy: routing network traffic to load-balanced services. Proxy must be present on every node. API object, DaemonSet
(doesn't work) kubectl get daemonSets --namespace=kube-system kube-proxy
Kubernetes DNS: naming and service discovery. Also run as replicated service. One or more DNS servers

-----

kube stuff again, then this https://www.youtube.com/watch?v=07jq-5VbBVQ

Google's Kubernetes Engine (GKE), but you could also take a look at Amazon (EKS) or Microsoft (AKS)
A 3 node kubernetes cluster in GKE (~5$/mo)
http://www.doxsey.net/blog/kubernetes--the-surprisingly-affordable-platform-for-personal-projects
https://cloud.google.com/kubernetes-engine/docs/quickstart
https://www.manning.com/books/kubernetes-in-action
https://github.com/kelseyhightower/kubernetes-the-hard-way


so, the ideal use case for k8s would be to set up these environments:
- dev
- load test
- production

do the development in the dev side, then using a command, sort of push the changes to the load test environment, where the webserver will be benchmarked to see the true performance, then after specific requirements are passed, push the changes to the production environment. Currently, if I were to manage all these by hand, it would be a nightmare, and people have indeed faced such nightmares in the past, but what's the point of suffering?

also a potential benefit of using k8s is that I can reboot computers once in a while, to sort of reset how much ram they will take, and there will be absolutely no down time. This is not the case with docker-compose and autostarting containers. It's a nice mechanism, but far from actually being zero down time. Actually, the idea to sort of resetting RAM is kinda a joke. I wasn't joking before, but the ridiculousness of it seems like a joke. Linux is sort of rock solid, especially kernel version 5. Like, there's pretty much nothing it can't deal with.

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml

kubectl run kuard --generator=run-pod/v1 --image=gcr.io/kuar-demo/kuard-amd64:blue
kubectl get pods
kubectl delete pods/kuard
kubectl run -d --name kuard --publish 8080:8080 gcr.io/kuar-demo/kuard-amd64:blue

http://api.k8s.kelvinho.org/api/v1/namespaces/kube-system/services/https://kubernetes-dashboard:/proxy
/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

so, this repo is the root of everything: https://github.com/kubernetes-up-and-running/kuard

so, interestingly, these are the kinds inside of the dashboard setup:
- Namespace
- ServiceAccount
- Secret
- ConfigMap
- Role
- ClusterRole
- RoleBinding
- ClusterRoleBinding
- Deployment
- Service
- Deployment

interesting reddit thread to go through: https://www.reddit.com/r/kubernetes/comments/b6zuuf/help_understanding_dashboard_permissions/
also possible dashboard uses in the future: https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

containers that fail liveness checks are restarted, containers that fail readiness checks are removed from service load balancers

probes/health checks:
- liveness, basic http, restarted when failed
- readiness, removed from service when failed
- tcpSocket, check if reachable
- exec probes, succeeds when script returns 0

applications running in the same pod share the same ip address and port space (network namespace), have the same hostname (UTS namespace), and can communicate using native interprocess communication channels over System V IPC (IPC namespace)

to add volume to a Pod manifest, 2 stanzas:
- spec.volumes: defines all volumes that may be accessed by containers in Pod manifest
- volumeMounts array: inside container definition, defines the volumes that are mounted into a particular container, and the path to mount onto

once a pod is scheduled to a node, no rescheduling occurs if that node fails. If I want to make multiple copies, then have to create it manually, which is undesirable. So use ReplicaSet instead

labels, identifying information
annotations, sort of labels, but can be used by libraries and whatnot. It's not like secrets, but act like 1

kubectl label deployments alpaca-test "canary=true". This is not recommended, because this is imperative, not procedural, and so kinda makes the point of K8S worthless. May be this will be a nice tool to quickly fix configs and stuff, but it just feels like a complete waste of K8S philosophy

---------------------------------------------

the k8s book, nigel

the vid thingy: https://www.pluralsight.com/courses/getting-started-kubernetes

"service meshes"? https://www.nginx.com/blog/what-is-a-service-mesh/

apiVersion: v1
kind: Pod
metadata:
  name: hello-pod
  labels:
    zone: prod
    version: v1
spec:
  containers:
  - name: hello-ctr
    image: nigelpoulton/k8sbook:latest
    ports:
    - containerPort: 8080

probes:
- startup
- readiness
- liveness

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-deploy
spec: most of the action will happen
  replicas: 10
  selector:
    matchLabels:
      app: hello-world
  minReadySeconds: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template: pod template the deployment will manage
    metadata:
      labels:
        app: hello-world
    spec:
      containers:
      - name: hello-pod
        image: nigelpoulton/k8sbook:latest
        ports:
        - containerPort: 8080

kubectl apply -f file.yml
kubectl apply -f {file}.yml --record
kubectl cluster-info --context kind-clu1
kubectl version --output=yaml
kubectl config view
kubectl config current-context
kubectl config use-context docker-desktop

kubectl describe {resource name} [{object name}] [-n {namespace}] [--all-namespaces]
kubectl get {resource name} [{object name}] [--watch] [-o yaml]
Resources:
- pods
- replicaset (rs). Only used when doing rollbacks
- service (svc)
- endpoints (ep)

objects:
- Deployments
- DaemonSets
- StatefulSets

kubectl describe secret -n kube-system | grep deployment -A 12

kubectl rollout history deployment hello-deploy
kubectl rollout status deployment hello-deploy
kubectl rollout
kubectl rollout undo deployment hello-deploy --to-revision=1

also it seems like etcd is what the master plane uses to sort of store config data. Etcd itself also seems to be decentralized

dashboard url: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
oh and, I can use SSH tunnel now, lmao. Ssh is so fucking useful and powerful

also see what's the right way to log into the dashboard? And see their security barriers

several types of services:
- ClusterIP: stable ip and port only accessible from inside the cluster. Guaranteed stable for the life of the Service. No need to touch iptables and ipvs rules
- NodePort: route traffic from outside to inside
- LoadBalancer: play nicely with commercial load balancing solutions out there
- ExternalName: route traffic to outside the cluster

Endpoints object, list of good pods

------------------------

kubernetes up and running, again

and yeah, I sort of feels like I was getting to that scale critical point where it's worth it to learn how to do this. This again, will be absolutely brutal, but just hang on with me, because it will be worth it and I can really focus on the real meat after this.

and again, I expect I can do load tests, complete load test suites somehow within K8S, so actively look that out

role-based access control. Yeah this sounds complicated. Seems to be exactly like ACL (Access Control List), like inside Magento's permission scheme
more specialized objects:
- DaemonSets, word play on ReplicaSets
- Jobs
- ConfigMaps, Secrets

5: Pods
6: Labels and annotations
7: Services
8: Ingress
9: ReplicaSets
10: Deployments
11: DaemonSets
12: Jobs
13: ConfigMaps, Secrets

oh and how can I get certificates when all of this stuff is happening?

oh and I get the feeling that I have to commit code, then it gets built, then it gets deployed, and that's time wasted.
docker-gc for garbage collecting

kubernetes-cni

on master:
kubeadm init --pod-network-cidr 10.244.0.0/16 \
  --apiserver-advertise-address 10.0.0.1 \
  --apiserver-cert-extra-sans kubernetes.cluster.home

so, pod network sort of can be any available ip address ranges. Then the advertise address is this machine (master)'s ip address

the above will print out a command for joining nodes. Hey, this is exactly like docker swarm. It's the same concept anyways. Looks like the below:
kubeadm join --token=<token> 10.0.0.1

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" >> /etc/apt/sources.list.d/kubernetes.list
apt update
apt-get install -y kubelet kubeadm kubectl kubernetes-cni

master node:
- eth0: 68.183.63.148
- eth1: 10.132.174.23/16
slave node:
- eth0: 142.93.73.82
- eth1: 10.132.55.18/16

kubeadm init --pod-network-cidr 10.132.174.0/8 --apiserver-advertise-address 10.132.174.23 --apiserver-cert-extra-sans k8s.cluster.home
systemctl daemon-reload

after updating /etc/sysctl.conf
sysctl --system

iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE
iptables -A FORWARD -i eth1 -o eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT
iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT

echo -e "\nexport KUBECONFIG=/etc/kubernetes/admin.conf">>~/.bashrc

kubeadm join 10.132.174.23:6443 --token atd6o8.lee8v9mcdhyusze1 \
    --discovery-token-ca-cert-hash sha256:6451ba36a605c57c6c896cb00ef02bd005db6d1fc203466cae4279f263c4c8a0 

kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml

https://www.weave.works/blog/introduction-to-kubernetes-pod-networking--part-1
https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy

dashboard: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc2/aio/deploy/recommended.yaml

ssh -L 8001:localhost:8001 root@68.183.63.148

ssh -L 8001:localhost:22 root@kelvinho.org
then, ssh root@localhost -p 8001, will get into the server

kubectl get DaemonSets --namespace=kube-system

default environment:
- DaemonSets:
  - calico-node
  - kube-proxy
- Deployments:
  - calico-kube-controllers
  - coredns
- Services:
  - kube-dns

--all-namespaces

kubectl apply -f <file name>
kubectl delete -f <file name>
kubectl delete <resource> <object>
kubectl label pods bar color=red
kubectl label pods bar color-
kubectl logs <pod-name>
kubectl exec -it <pod name> -- bash (well this is inconvenient lmao)
kubectl port-forward <pod-name> 8080:80, forward traffic from 8080 to 80 on the remote server

kubectl run kuard --generator=run-pod/v1 --image=gcr.io/kuar-demo/kuard-amd64:blue
kubectl delete pods/kuard

