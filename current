
so, priorities now:
- read rocket propulsion elements
- file:///C:/Users/Lenovo-1/Downloads/Ebook/Warehouse%20Management_%20A%20Complete%20Guide%20to%20Improving%20Efficiency%20and%20Minimizing%20Costs%20in%20the%20Modern%20Warehouse-Kogan%20Page%20(2014).pdf
- file:///C:/Users/Lenovo-1/Downloads/Christoph%20Roser%20-%20_Faster,%20Better,%20Cheaper_%20in%20the%20History%20of%20Manufacturing_%20From%20the%20Stone%20Age%20to%20Lean%20Manufacturing%20and%20Beyond-Productivity%20Press%20(2016).pdf
- book: "pmi-pmbok", "pmbok guide"

masterclass.comds

chapter 11 of the bankruptcy code

effective altruism global:
- https://www.eaglobal.org/events/ea-global-2017-uk/
- https://www.youtube.com/channel/UCEfASxwPxzsHlG5Rf1-4K9w

can RL agents always be reasonable when framerate switches to 30fps to 60 or higher? If they are influenced by this then it seems like the error signal attenuation still applies right? So at that point, why not just create a much simpler policy-only optimization process?

also, pay very, very close attention to human's failure modes as well, as I don't think this has been developed in the community yet

again x3, I think I can implement this whole thing without the formulation of action-state values or state values. So try out that kind of simple tree search. Also I think by creating all of the infrastructure on my own, I can really provide a new perspective in the game

tiny bit of math and workable code: https://towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2
env: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py
more complex running code: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html

look up q learning on youtube or something

also see how humans actually do RL too. How can they correlate long distance reward signals? Are they even good at such a task? It seems like I can somehow use attention as this mechanism that spans multiple time frames, so as to make the RL capabilities more pronounced, as that acts kinda like a skip connection

so the model-based RL agents seems interesting, because it can do fundamental experiments in the world to build an accurate model, then use that model as a proxy for the real world, then do experiments on them, instead of on the real world

do the predicting distances by being consistent thingy, because it's a real possibility that may reveal more what's really going on

https://www.techrepublic.com/article/natural-language-processing-a-cheat-sheet/

how do ppl actually build chatbots from language models? In other words, how to use language models flexibly and in 1 domain. "how chatbots are made"

"neural turing machine" sounds very, very impressive, so look into that

https://arxiv.org/pdf/1709.08568.pdf

what does it mean for bert to be bidirectional? I mean, aren't attention mechanism don't have a direction?

superglue: a stickier benchmark for general-purpose language understanding systems: https://w4ngatang.github.io/static/papers/superglue.pdf
original glue benchmark: https://openreview.net/pdf?id=rJ4km2R5t7

"feature attribution, I guess they compute gradients all the way to the input and use that to construct heatmaps of which voxels are most important for a classification". Actually, the heatmaps can be generated with the last CNN layer before the activation. Anyway, bunch of links:
- https://distill.pub/2020/attribution-baselines/
- https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview
- (2018) https://mlconf.com/sessions/interpretability-beyond-feature-attribution-quant/
- https://deep.ghost.io/simple-feature-attribution/
- https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d
- interpretability beyond feature attribution: quantitative testing with concept activation vectors: https://arxiv.org/pdf/1711.11279.pdf

"robustify black-box controllers from neural nets"

the early phase of NN training: https://arxiv.org/pdf/2002.10365.pdf
deep RL from human preferences: https://arxiv.org/pdf/1706.03741.pdf, sounds familiar
the lottery ticket hypothesis: finding sparse, trainable NNs: https://arxiv.org/pdf/1803.03635.pdf
planning chemical syntheses with deep NNs and symbolic AI: https://www.nature.com/articles/nature25978, alphago-inspired architecture
scaling laws for neural language models: https://arxiv.org/pdf/2001.08361.pdf
learning the difference that makes a difference with counterfactually-augmented data: https://arxiv.org/pdf/1909.12434.pdf
linformer: self-attention with linear complexity: https://arxiv.org/pdf/2006.04768.pdf

https://venturebeat.com/2020/07/15/mit-researchers-find-systematic-shortcomings-in-imagenet-data-set/

distillation & amplification:
- alphago zero and the foom debate: https://intelligence.org/2017/10/20/alphago/
- https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446
- https://ai-alignment.com/benign-model-free-rl-4aae8c97e385
- https://ai-alignment.com/policy-amplification-6a70cbee4f34
- "capability vs policy amplification"
- https://intelligence.org/2018/05/19/challenges-to-christianos-capability-amplification-proposal/
- https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616

read more into inverse RL, seems like a very promising way to go about this

https://intelligence.org/all-publications/

ways to meaningfully contribute to ai safety? Doesn't seem there's a way besides actually building the system

read over his papers to know which areas is he working in: http://www.pages.drexel.edu/~ek826/, http://www.pages.drexel.edu/~ad3639/
- https://openaccess.thecvf.com/content_CVPR_2020/papers/Kim_Modeling_Biological_Immunity_to_Adversarial_Examples_CVPR_2020_paper.pdf
- apparently, he worked on the loihi chip too, and ask about the nitty gritty details on SNNs
- is it easy now to get started on spiking NNs? doesn't seem so
- is there research left for small labs with may be 4 V100
- funding source???

read over russell's papers: https://people.eecs.berkeley.edu/~russell/research/future/

https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/
https://towardsdatascience.com/language-models-and-fake-news-the-democratization-of-propaganda-11b1267b3054

"expert iteration", thinking fast and slow with DL and tree search: https://arxiv.org/pdf/1705.08439.pdf
neural architecture search with RL: https://arxiv.org/pdf/1611.01578.pdf
neural architecture search: a survey: https://arxiv.org/pdf/1808.05377.pdf

turn faces into anime girls. May be real guy -> anime guy/girl, and real girl -> anime guy/girl as well. Devise a way of switching the network back and fourth, to practice inputting in explicit meaning. This should be the first step in actually extracting information out of a system

can RL from pixels be as efficient as RL from state? https://bair.berkeley.edu/blog/2020/07/19/curl-rad/

discovering RL algorithms: https://arxiv.org/pdf/2007.08794.pdf

pipeline stuff (the concurrent idea in CPUs and stuff): https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf

squad:
- know what you don't know: unanswerable questions for squad: https://arxiv.org/pdf/1806.03822.pdf
- squad: 100k+ questions for machine comprehension of text: https://nlp.stanford.edu/pubs/rajpurkar2016squad.pdf

paragraph stuff, "paragraph recognition:
- http://www.tbluche.com/files/Goog2017.pdf
- scan, attend, and read: end-to-end handwritten paragraph recognition with MDLSTM attention: https://arxiv.org/pdf/1604.03286.pdf

covid stuff:
- https://connect.biorxiv.org/relate/content/181
- continual bert: continual learning for adaptive extractive summarization of covid-19 literature: https://arxiv.org/pdf/2007.03405.pdf
- a survey on applications of AI in fighting against covid-19: https://arxiv.org/pdf/2007.02202.pdf
- IoT for current covid-19 and future pandemics: an exploratory study: https://arxiv.org/pdf/2007.11147.pdf

again with the image segmentation but different color for different objects of same class, "instance image segmentation"

deepmind's DNC (differential neural computer), mentioned by goertzel

one weird trick for parallelizing CNNs: https://arxiv.org/pdf/1404.5997v2.pdf
http://course18.fast.ai/part2.html
dawn bench: an end-to-end DL benchmark and competition: https://dawn.cs.stanford.edu/benchmark/papers/nips17-dawnbench.pdf
why train what you can code? Rekall: a compositional approach to video analysis: https://dawn.cs.stanford.edu/2019/10/09/rekall/
infrastructure for usable ML: the stanford dawn project: https://arxiv.org/pdf/1705.07538.pdf
http://image-net.org/update-sep-17-2019
"resnet inference energy cost"
noscope: optimizing NN queries over video at scale: https://arxiv.org/pdf/1703.02529.pdf
hidden technical debt in ML systems: https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf
E^2-Train: training SOTA CNNs with over 80% energy savings: https://arxiv.org/pdf/1910.13349.pdf
ML: the high interest credit card of technical debt: https://research.google/pubs/pub43146/, https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43146.pdf
"functional vigilance of drivers while using autopilot", lex's MIT research on autopilot

attention and its relation to event-based systems?

so the idea GPT-3 is pushing is that when the big network gets fed a prompt, it forms this little network on the go that's specialized to handle the task, which sort of explains why GPT-3 needs to be so big, and it also sort of explains the power-law relationship. So really reason about the relative sizes of these networks. How large is the actual specialized network and the full one? Then make comparisons to other animals

deepmind, deep RL and its neuroscientific implications: https://deepmind.com/research/publications/Deep-Reinforcement-Learning-and-its-Neuroscientific-Implications
yoshua bengio: from system 1 DL to system 2 DL: https://www.youtube.com/watch?v=T3sxeTgT4qc
seems interesting: https://www.wandb.com/papers
don guy seems interesting: https://www.youtube.com/playlist?list=PLpP2qGSxCw-e0nMetkx41JVrdcivVxUPm, https://www.pugetsystems.com/all_hpc.php
obama interview: https://www.youtube.com/watch?v=Tjl8ka3F6QU

spectral normalization, seems to fix problems with the discriminator

https://www.fast.ai/2018/07/02/adam-weight-decay/

path from now? Short and long term goals?

long term goal is to be able to:
- show others where is this technology at, what can be done with it, how to think about our future and how to be prepared for it
- understand more about how the brain actually works
- apply image supervised learning in all other domains
- do research on my own with language models
- apply collaborative filtering and dealing with tabular data at scale, and active learning too
- paragraph recognition
- object recognition
- do research on RL
- accurately model physics

short term goal is to:
- read over all papers currently, which is quite a lot
- do all brilliant stuff (by this week)
- do all jeremy howard stuff
- do a few kaggle stuff
- warn teachers of GPT-3, create an interface to automatically generate scripts to amaze them

https://medium.com/dataseries/google-deepminds-dreamer-is-a-reinforcement-learning-agent-that-can-solve-long-horizon-tasks-5faa6f6b63b
https://singularityhub.com/2020/07/26/deepminds-newest-ai-programs-itself-to-make-all-the-right-decisions

really do the image label from minh thingy, to prove out everything I've picked up

also wait, inference energy cost on GPT-3 seems to be quite low (4kWh/100 pages of output). Still 2-3 orders of magnitude away from human brains, but it's actually low enough to be beneficial. So, the bulk energy cost is in the training, where we're essentially doing the evolution thingy again

proximal policy optimization, another popular RL method

intelligence explosion microeconomics: https://intelligence.org/files/IEM.pdf

https://www.lesswrong.com/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai
https://www.lesswrong.com/posts/Mzrs4MSi58ujBLbBG/you-can-probably-amplify-gpt3-directly

breaking the quadratic attention bottleneck in transformers: https://www.reddit.com/r/MachineLearning/comments/hxvts0/d_breaking_the_quadratic_attention_bottleneck_in/

reddit carreer in researching AGI

again, think about how to structure an active learning problem to create a fast feedback system. Basic principles should be fairly simple and easy to see, although there need not be actual implementable code to extend from. And yeah, I feel like there is a fear of generating new samples on the go and so ppl are turned away by that and don't necessarily embrace the nonlinearity of the real world

https://mailchi.mp/2485e6b42012/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals-for-ai-alignment?e=e50845312e

sparse coding:
- http://www.scholarpedia.org/article/Sparse_coding
- https://blog.metaflow.fr/sparse-coding-a-simple-exploration-152a3c900a7c
- http://ufldl.stanford.edu/tutorial/unsupervised/SparseCoding/
- https://www.cs.ubc.ca/~schmidtm/MLRG/sparseCoding.pdf
- https://www.sciencedirect.com/topics/engineering/sparse-coding
- http://users.ics.aalto.fi/harri/dityo/node3.html
- https://stats.stackexchange.com/questions/118199/what-are-the-differences-between-sparse-coding-and-autoencoder
- https://www.youtube.com/watch?v=7a0_iEruGoM

how can RL deal with a cold start

ddpg algorithm

world models: https://arxiv.org/pdf/1803.10122.pdf

https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion
https://www.kaggle.com/twinkle0705/your-starter-notebook-for-osic#Reading-the-data

https://www.reddit.com/r/MachineLearning/comments/g16s40/r_metalearning_in_neural_networks_a_survey/

https://towardsdatascience.com/visual-attention-model-in-deep-learning-708813c2912c
https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html

https://dailynous.com/2020/07/30/philosophers-gpt-3/

https://mml-book.github.io/book/mml-book.pdf
http://www.arxiv-sanity.com/

crafting papers on ML: https://pdfs.semanticscholar.org/3efc/b97c1de1c87832a7a1d99e91801992a938ec.pdf
http://joschu.net/blog/opinionated-guide-ml-research.html
ask reddit how to approach the conversation to ppl about AI, "reddit how to discuss impact of machine learning"

so temporal-difference is basically just making sure the internal model of reality is good?

https://www.cs.cmu.edu/afs/cs/academic/class/15883-f15/slides/

scikit-learn is mentioned a whole lot, so what does it actually offer?

agi stuff:
- http://www.hutter1.net/ai/introref.htm
- https://www.youtube.com/channel/UCCwJ8AV1zMM4j9FTicGimqA
- https://goertzel.org/agi-curriculum/
- https://cis.temple.edu/~pwang/AGI-Intro.html
- https://en.wikipedia.org/wiki/Artificial_intelligence
- https://www.reddit.com/r/artificial/comments/6cnlr6/monthly_how_to_get_started_with_ai_thread/
- http://www.agi-society.org/membership/
- https://content.sciendo.com/view/journals/jagi/jagi-overview.xml

https://placements.openmined.org/

http://worrydream.com/MagicInk/
"how much computation the brain does"
- https://www.cs.cmu.edu/afs/cs/academic/class/15883-f15/slides/
- https://www.cs.cmu.edu/afs/cs/academic/class/15883-f15/slides/marr.pdf
- http://www.cs.utexas.edu/~dana/Book1.pdf
- https://www.frontiersin.org/articles/10.3389/fnsys.2016.00095/full

meta learning through hebbian plasticity: https://www.youtube.com/watch?v=v2GRWzIhaqQ

https://www.reddit.com/r/MachineLearning/comments/i9kztq/d_hidden_gems_and_underappreciated_resources/

http://www.argmin.net/2018/06/25/outsider-rl/
http://karpathy.github.io/2016/05/31/rl/
lex francois round 2

ask about RL training stability to that redditor

jeremy has a new DL course in 2020 so check that out: https://www.youtube.com/watch?v=bHVqO5YyNbU

cashapp

may be deep RL introspection in our current systems help us reason about this take off scenario better

on the measure of intelligence: https://arxiv.org/pdf/1911.01547.pdf

https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce
https://towardsdatascience.com/summary-of-tabular-methods-in-reinforcement-learning-39d653e904af

also something seems wrong. Why is the hippocampus so small and the cortex so large? Also why the cerebelum is so dense and the cortex so sparse? And what's the energy efficiency (Landauer's limit) of different parts of the brain?

DJ SEO's neural dust: https://arxiv.org/pdf/1307.2196.pdf
neosensory throughput: http://ww-w.eagleman.com/papers/novich_eagleman_2015.pdf

https://www.acrobiosystems.com/P3103-SARS-CoV-2-%28COVID-19%29-S1-protein-His-Tag.html

does catastrophic forgetting occur in RL systems?

pick up an epidemiology book

this as reference for the NMRI model: https://deepmind.com/blog/article/neural-scene-representation-and-rendering

domain experts:
- https://www.kaggle.com/anarthal/dicom-metadata-eda
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/165727
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/166123
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/165253
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/169658
top:
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/169121
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/164870
- https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/data

python pil 3d projections
https://stackoverflow.com/questions/39945068/python-pil-3d-projection, lmao, so no one even cares about this???

seems like no one is talking about the neocortex, aka ppl have no idea how it actually works

with 3d stuff: https://www.kaggle.com/gzuidhof/full-preprocessing-tutorial
more brain 3d stuff: https://www.youtube.com/channel/UCDYTPnuWBAVsOVGJ0LTmTgw/featured

https://www.youtube.com/c/Brainbook/videos

dicom think about unit tests for data coming in, so that I won't be surprised?
dicom also verify ImageOrientationPatient with 3d viz

https://distill.pub/2017/aia/
http://lernapparat.de/

another leslie paper: https://arxiv.org/pdf/1506.01186.pdf

https://www.fast.ai/2018/04/30/dawnbench-fastai/

"wasserstein distance": GAN: https://arxiv.org/pdf/1701.07875.pdf

"ulmfit vs bert"
https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html
http://jalammar.github.io/illustrated-bert/

https://en.wikipedia.org/wiki/Mechanotransduction

nanoparticle fabrication: https://www.researchgate.net/publication/226347418_Nanoparticle_Fabrication

alertness, arousal, attention, awareness

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216789

organic chem, "steric hindrance":
- https://www.masterorganicchemistry.com/2012/06/05/nucleophiles-and-electrophiles/
- https://www.chemistrysteps.com/sn1-sn2-e1-e2-choose-which-mechanism/

https://read.amazon.com/?asin=B014TMUFX6
https://en.wikipedia.org/wiki/Epigenetics
https://github.com/gordicaleksa/pytorch-original-transformer

https://syncedreview.com/2020/11/06/deepmind-proposes-graph-theoretic-investigation-of-the-multiplayer-games-landscape

cortical columns: https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00186/full

reticular activating system
galanin
arcuate fasciculus

- integrative analysis of 111 reference human epigenomics: https://www.nature.com/articles/nature14248
- FTO obesity: https://www.nejm.org/doi/full/10.1056/NEJMoa1502214

https://www.reddit.com/r/science/comments/4pmivr/science_ama_series_im_manolis_kellis_a_professor/
irx3 and irx5 genes: https://en.wikipedia.org/wiki/IRX3, https://en.wikipedia.org/wiki/FTO_gene
prdm9: https://en.wikipedia.org/wiki/PRDM9

cortex layer 5
5ht 2a/2c receptors: https://en.wikipedia.org/wiki/5-HT2A_receptor

https://www.researchgate.net/publication/317841703_The_oldest_fossil_mushroom
https://www.researchgate.net/publication/343188979_Fungi_in_the_rear_mirror_A_brief_history_of_the_fungi_during_the_last_two_billion_years

pfizer vaccine paper: https://www.medrxiv.org/content/10.1101/2020.06.30.20142570v1

transcranial magnetic stimulation
fusiform face area
middle temporal visual area (V5)

16 psyche
making sense of pharmacology: inverse agonism and functional selectivity, https://academic.oup.com/ijnp/article/21/10/962/5066769
drug design and discovery: principles and applications: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6155886/
could a neuroscientist understand a microprocessor: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268
https://www.sciencemag.org/news/2020/11/game-has-changed-ai-triumphs-solving-protein-structures
https://www.nih.gov/coronavirus
https://www.genome.gov/about-genomics/fact-sheets/Sequencing-Human-Genome-cost
https://www.nature.com/scitable/topicpage/the-role-of-methylation-in-gene-expression-1070/
architecture of sars-cov-2 transcriptome: https://www.sciencedirect.com/science/article/pii/S0092867420304062
d614g mutation: https://www.nature.com/articles/s41586-020-2895-3
entire covid 19's genome: https://www.ncbi.nlm.nih.gov/gene/43740568

https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5641975/
https://pubmed.ncbi.nlm.nih.gov/25561484/

curious stuff:
- https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/
- https://vimeo.com/238243932
- https://www.ijcai.org/Proceedings/2017/0344.pdf
- https://arxiv.org/pdf/1606.01868.pdf
- https://www.youtube.com/watch?v=0yI2wJ6F8r0
- rainbow: https://arxiv.org/pdf/1710.02298v1.pdf

other interesting notes:
- a lot of times the network is stuck because oscillation frequency is too high. What is observed is the actions' oscillations seem to coincide with the locations' oscillations, but somehow the phase is just off and leave the system oscillating forever
- can it be that the fourier thingy sort of hides a prior behind it to make it oscillate a lot. To test this out, try this with other environments

https://www.chemistrysteps.com/sn1-sn2-e1-e2-choose-which-mechanism/

that combined knowledge thing:
https://deepmind.com/blog/article/neural-scene-representation-and-rendering
https://science.sciencemag.org/content/sci/360/6394/1204.full.pdf?ijkey=kpkRRXA1ckHD6&keytype=ref&siteid=sci

https://en.wikipedia.org/wiki/Homeobox

https://en.wikipedia.org/wiki/Schlenk_line

tesla's quarterly reports?

mass production: https://www.reddit.com/r/Entrepreneur/comments/9n8xyj/inventrepreneurship_i_took_an_idea_to_mass/

also do a youtube series on the RL stuff so far

https://en.wikipedia.org/wiki/Proteostasis
melanoma brain metastases

temp discarded:
- https://www.kaggle.com/nxrprime/fibrosis-eda-fast-ai
- https://www.kaggle.com/dipam7/intel-scene-progressive-image-resizing

https://en.wikipedia.org/wiki/Structural_alignment

ACE2, much more than just a receptor for sars-cov-2: https://www.frontiersin.org/articles/10.3389/fcimb.2020.00317/full
mRNA closed loop model: https://www.cell.com/molecular-cell/fulltext/S1097-2765(18)30941-9
anti-ACE2 antibodies??? https://www.rndsystems.com/resources/articles/ace-2-sars-receptor-identified

https://joininteract.com/fellowship/

etsy and https://www.easyship.com/blog/ecommerce-sites-for-wholesalers

ttt diagrams: https://www.tf.uni-kiel.de/matwis/amat/iss/kap_8/illustr/s8_4_3a.html

https://en.wikipedia.org/wiki/DNA_nanotechnology
https://en.wikipedia.org/wiki/Molecular_self-assembly
https://en.wikipedia.org/wiki/Non-contact_atomic_force_microscopy

goals for this trip?
- demonstrate plasmid cloning. Heard it's super simple and whatnot
- understand the compliment system designs, and see if I can come up with comparable systems
- create a system that accurately maps out distances of a camera feed when also given the coordinate system
- do a bunch of comparative genomics. Basically figure out the story of every life on earth. Also try to run AlphaFold2. Also try to figure out the energy landscape in all of this. Targets:
	- DNA polymerase
	- cap binding complexes
	- ribosome subunits
	- pseudouridine synthase: Z71568, YNL292w, https://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi?seqinput=18158779
	- mitochondria, atp synthase: NC_012920, 251831106
	- methane monooxygenase: CH4 + O2 + NAD(P)H + H+ -> CH3OH + NAD(P)+ + H2O, single bond replacements
	- amylase, alcohol dehydrogenase, hemoglobin, fibrin, collagen, actin, insulin, potassium channels, luciferase, hemagglutinin, HIV-1 protease, p53 tumor suppressor, cellulases
- try to infect cells with modified virus with a specific payload
- actually try to run alphafold: https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13

https://www.chems.msu.edu/resources/tutorials/ASPEN

https://en.wikipedia.org/wiki/Contig
https://en.wikipedia.org/wiki/Scaffolding_(bioinformatics)

Transcriptome Shotgun Assembly, context: https://www.ncbi.nlm.nih.gov/genbank/release/240/

covid genome: https://www.ncbi.nlm.nih.gov/nuccore/NC_045512
covid characteristics: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7402395/
aryl: https://www.ncbi.nlm.nih.gov/protein/NP_001612.1

YP_009724389.1

https://en.wikipedia.org/wiki/Signal_peptide
https://en.wikipedia.org/wiki/Secretion
https://en.wikipedia.org/wiki/Protein_targeting#Protein_translocation
https://en.wikipedia.org/wiki/Post-translational_modification
refseq: https://www.ncbi.nlm.nih.gov/books/NBK21091/
https://en.wikipedia.org/wiki/Molecular_machine
https://en.wikipedia.org/wiki/Human_mitochondrial_genetics
https://en.wikipedia.org/wiki/TIM_barrel
https://en.wikipedia.org/wiki/Physarum_polycephalum
https://en.wikipedia.org/wiki/RNA-binding_protein
https://en.wikipedia.org/wiki/Origin_of_replication
sample complexity: https://en.wikipedia.org/wiki/Dihydrofolate_reductase
https://en.wikipedia.org/wiki/High_throughput_biology
https://en.wikipedia.org/wiki/Proteome
https://en.wikipedia.org/wiki/Luciferase
https://en.wikipedia.org/wiki/Pattern_recognition_receptor
https://en.wikipedia.org/wiki/Cambrian

also, what's up with the polyprotein nature of covid? Actually you know what, I heard many proteins are like that too, like histones, or hemoglobin, or atp synthase, so figure out the general dynamics of these systems

https://fold.it/portal/node/2010076
https://fold.it/portal/node/2007798
https://www.reddit.com/r/MachineLearning/comments/k4n3m2/d_deepminds_alphafold_2_explained_ai_breakthrough/

go over 23andme analysis

https://www.biorxiv.org/content/10.1101/2020.10.31.362848v2

subgenual cingulate cortex

old aging stuff:
- https://journals.physiology.org/doi/full/10.1152/ajpheart.00287.2018?rfr_dat=cr_pub++0pubmed&url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&
- https://sci-hub.scihubtw.tw/10.1007/s10522-006-9043-9

https://en.wikipedia.org/wiki/Quantum_dot

how the spike protein works: https://www.nature.com/articles/s41594-020-0479-4

actually try to implement the attention idea that creates consciousness

also see how advanced stuff-pickup programs are, and can they do stuff really freeflow?

invest in rocket lab

novozyme, research lab that produces industrial enzymes, microorganisms & biopharmaceutical ingredients. Not billion dollars yet, just on the million dollar scale

epimap

https://en.wikipedia.org/wiki/Genome-wide_association_study

discovery path for proteins? Where are the entropy leaks?

https://en.wikipedia.org/wiki/Transcription_factor
https://en.wikipedia.org/wiki/Transfection

try to create an agent that has some sense of higher dimensions

http://www.aosabook.org/en/index.html

stuff I really shouldn't put in nodes cause I want to do them so bad:
- https://people.cs.umass.edu/~pthomas/courses/CMPSCI_687_Fall2020/687_F20.pdf
- https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html

https://en.wikipedia.org/wiki/Mitochondrial_replacement_therapy

https://en.wikipedia.org/wiki/Structural_genomics
https://en.wikipedia.org/wiki/Structural_biology

https://ocw.mit.edu/courses/chemistry/5-08j-biological-chemistry-ii-spring-2016/

limited memory: https://arxiv.org/abs/2002.05645v5
dyson sphere calculations: https://arxiv.org/pdf/2006.16734.pdf

manolis stuff: https://twitter.com/manoliskellis/status/1408396053741674497

graph nns, then try to tackle some chemistry problems

https://en.wikipedia.org/wiki/Danger_model

https://viblo.asia/p/deep-learning-graph-neural-network-a-literature-review-and-applications-6J3ZgP0qlmB

actually understand affine transforms in pytorch

also, try to apply attention stuff to that gravity prediction problem earlier. Have a great feeling this will work

also, several ideas for representing chemical structures:
- smiles (are you kidding me?)
- plain 2d image, and use convs
- morgan fingerprints, best used with random forests or sth like that
- my idea, kinda like an extended version of gravity attention thingy, but with fixed length features coming from variable length tokens through multiple layers
- this is actually golden. Just simple chemical domain classification (kidney/tooth/CNS/...), but does demonstrate good ability to represent chemicals: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236#
some lackluster papers:
- https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2523-5
- file:///home/kelvin/Downloads/computers-10-00074.pdf
- DRL, also seems golden: https://pubs.acs.org/doi/full/10.1021/acscentsci.7b00492

remaining model stuff:
- modelling kerosene combustion: https://arxiv.org/pdf/2003.00428.pdf
- nn in chemistry: http://web.uni-plovdiv.bg/plamenpenchev/mag/files/ang_chem2.pdf
- alphafold2 paper: https://www.nature.com/articles/s41586-021-03828-1
- dimensionality reduction in neural data analysis: https://xcorr.net/2021/07/26/dimensionality-reduction-in-neural-data-analysis/

talking about fingerprints, there are a few ideas:
- https://www.rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf
- https://towardsdatascience.com/random-matrix-theory-the-best-classifier-for-prediction-of-drug-binding-f82613fb48ed
- predefined features (human made), binary bit has it or not
- https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00445-4

derive heart from arccos again

rna therapeutics overview: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4743652/

shelly plug

aav:
- https://ashpublications.org/blood/article/122/1/23/31570/Immune-responses-to-AAV-vectors-overcoming
- https://www.frontiersin.org/articles/10.3389/fimmu.2020.00670/full
- https://www.vectorbiolabs.com/

left on mess:
- https://rupress.org/jem/article/216/5/1005/121029/Microbial-therapeutics-New-opportunities-for-drug
- https://blogs.imf.org/2021/04/29/achieving-the-sustainable-development-goals-will-require-extraordinary-effort-by-all/
- https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853
- https://www.thermofisher.com/sg/en/home/life-science/cloning/cloning-learning-center/invitrogen-school-of-molecular-biology/molecular-cloning.html
- https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/
- https://www.nature.com/articles/s41467-021-24525-7
- https://theaisummer.com/deep-learning-biology-alphafold/
- https://t.co/MLwa8O8zNt?amp=1
- https://twitter.com/Thom_astro/status/1427635722542927872?s=07
- ML pitfalls: https://arxiv.org/abs/2108.02497
- https://www.ncbi.nlm.nih.gov/books/NBK9839/
- https://www.eetimes.com/micro-magic-risc-v-core-claims-to-beat-apple-m1-and-arm-cortex-a9/

tesla multiscale features uses bifpn, so what's that about?

website module automate pull text from sites

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1463026/
https://en.wikipedia.org/wiki/RNA-dependent_RNA_polymerase
https://www.stateof.ai/
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5718548/
https://www.science.org/content/article/new-sars-cov-2-variants-have-changed-pandemic-what-will-virus-do-next
https://journals.physiology.org/doi/full/10.1152/ajpheart.00287.2018?rfr_dat=cr_pub++0pubmed&url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7217285/
https://arxiv.org/pdf/1812.08434.pdf
https://blog.addgene.org/plasmids-101-restriction-cloning
https://en.wikipedia.org/wiki/Spirometry

from skimage import measure, morphology
measure.marching_cubes
skimage.morphology

nice: https://www.youtube.com/c/jacksonlaboratory/videos

https://blog.gregbrockman.com/define-cto-openai
https://openai.com/blog/more-on-dota-2/#infrastructure

https://www.faa.gov/space/stakeholder_engagement/spacex_starship/media/Summary_Draft_PEA_for_SpaceX_Starship_Super_Heavy_at_Boca_Chica.pdf

printing services:
- https://www.fastradius.com/capabilities/additive-manufacturing
- https://en.wikipedia.org/wiki/3D_printing_processes

applyMp 10000 processes each doing 1 task, or 100 processes, each doing 100 tasks?

document graphEqn, and show examples where it's useful

rename to gE again, and have something like gE(a = gE.Var()), so that variable names can be captured. Also do the multiple expression thingy, may be .also(). The vision is to just multiply stuff at will, then check a visualization graph to see how many source variables are left. Then we can manually set those, and every value will just pop into life!

https://twitter.com/DeepMind/status/1440710585964916741

figure out what lnav does

talk about accuracy of derivative and inverse functions

applyS as decorator?

upload graphEqn lol

actually do resume stuff
