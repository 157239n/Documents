
This is an excerpt from a youtube comment of mine. Feels like this sort of gives a clear and strong path

Well, so the context is snowden and basically how computers work and how do they work together. I used to basically try really hard to understand these things few years ago, like trying to buy a domain, trying to set up wamp and whatnot, but I sort of still lacking foundational knowledge. Then I remember reading over exactly this link (https://www.digitalocean.com/community/tutorials/how-to-install-linux-apache-mysql-php-lamp-stack-ubuntu-18-04) and everything just clicks together, so I hope that you will be able to learn through that.


But on the bigger picture:


Top-notch resources, these are quite rare and I think they are so damn good at conveying information. I call them "gems":
- Learn computer's fundamentals, even to the barebones electronics. This is quite hard, without a top-notch resource. But luckily, there is one that I know of which is Crash Course Computer Science series (https://www.youtube.com/watch?v=tpIctyqH29Q&list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo). I highly recommend episodes 1 through 9: Advanced CPU Designs. After that the series is still good but I think is not fundamental
- Then may be learn something like Processing (home website at processing.org), a programming language based on Java, but gets rid of all the annoying stuff and is entirely focused on art. Processing also has python mode (branded "processing.py") and javascript mode (branded "p5.js") if you're more comfortable with those 2, but I still recommend the vanilla Processing based on Java. One of the founders of Processing is very active in tutorials and whatnot, his name is Daniel Shiffman. Few years ago he had a book called "The nature of code", which I believe is free and I highly suggest you go through that, cause it's so mind blowing, the stuff that Dan showed. Also Dan have been doing a series called "The coding train" on youtube. I have not go through those videos to know whether they're a gem or not, but may be it helps to go along with the book. Also I have to mention that Dan has also made "The nature of code" into a video series (in case reading a book is boring to you), look it up. May be less of a gem is khan academy, because I came to Processing from khan. It was okay, it's not as a gem as Processing, but if some stuff you don't understand, then may be looking at this is a good idea?
- Also while you are learning, aim to understand this talk: The tragedy of systemd (https://www.youtube.com/watch?v=o_AIw9bGogo). This is a very, very technical talk that mentions a lot of interesting things. You will not understand after 3 minutes and this is in no way for beginners, but it gives a high-in-the-sky goal and it gives interesting words for you to look up on google.
- Also learn to program stuff nicely. Typical bootcamps are fine, code.org is fine, khanacademy is fine, but like, this series of articles from MIT really gives content and no bullshit: https://ocw.mit.edu/ans7870/6/6.005/s16/. In case the link is broken, this is MIT's 6.005: Software Construction course. This covers pretty much everything in regards to how to program stuff to be maintainable, free from bugs. This skips all the bullshit of bootcamps and gets real with how software is actually built professionally.
- DigitalOcean documentation. There are a bunch of services they provide, but the most important one is their Virtual Private Server (VPS) which they rebranded as "Droplets". Other companies also provide VPS services, but to my mind no other company has documentation as straight to the point as DigitalOcean. Once you have finished with droplets (long time after you have first used a droplet, think months to 1-2 years), then you can explore other stuff DigitalOcean provides, like Domain/DNS, Clusters, K8S, Load Balancers, Spaces. Of all of that I think you should start with Spaces. But again, by this time, you will likely know where to advance to next. Last point is that these services cost money. The cheapest Droplet they offer is $5/month, which I think is totally worth it, considering the amount of stuff you will be able to learn.


Some general advice:
- Aim to learn things fundamentally. Have an attitude to learn a whole subject rather than following some tutorial for specific case that solves your problem. Of course sometimes you just want to get the job done, but like, if you can choose between learn the whole thing and apply a quick fix, choose to learn the whole thing/subject.
- Also I see there are a lot of movement going to languages like javascript and python, and they are the shinier languages. You can learn that, but I would suggest you learn these 3 fundamental languages: C++ (close to machine), Java (middle ground), and Python (not close to machine and abstract). These 3 languages span most of other languages, in code functionality and style. Like, C# looks, feels, runs like Java. Pascal, C and Go looks feels, runs like C++. Also for syntax, C, C#, C++, Java, JavaScript, PHP look really similar to each other and these languages basically run everything nowadays. Java is used most in the world, C second, C++ third, Python fourth (last time I remembered). PHP runs 80% of websites. So basically, once you learn to look at C-like code, you can basically adapt to any other C-like languages (C-like at the syntax only, not the functionality). Anyway, after doing Processing, start to learn more on the 3 core languages of C++, Java and Python.


Also some pitfalls to avoid:
- Don't use windows. This is, like damn, so fundamental. I started out using windows, like everyone else, and I like it really much, but windows normally abstracted away from what is actually going on in a computer so much. Like, I used to try writing visual basic programs, but it feels like I'm learning how to use microsoft's tool, rather than using the underlying technology and actually understanding them. Also, in the 1970, there was the Unix operating system, and basically 100% of computers ran it (also note, don't use/learn unix, it's really bad nowadays and counterintuitive, although it sounds awesome the way I'm describing it). Then later on macoS takes inspiration from unix, then linux. Then android takes inspiration from linux and ios takes inspiration from macos. So basically, all of these operating systems are more than 90% similar and all are unix-like. So a lot of research and development of like the internet, they happened on unix-like OSes and so, if you learn about any unix-like os, it will feel much more natural and easy to understand. Also unix's philosophy is to sort of be brutally simple, and build functionality on existing programs/ideas, so it will feel much more logical too. That said, I still use windows for cat videos, reddit and whatnot. I just like windows more, but for technical stuff, I strongly recommend learning linux. If you have a mac then it's more or less the same, but anyway, stick to unix-like OSes for technical stuff.


These should get your mind blown several times :)) And if you get stuck just ask someone (most likely searching on google and stack overflow, rather than actually asking someone), or just ask me here again


Edit, if you don't know what Linux to choose from, just go with Ubuntu, no questions asked. But here are more distros for consideration (but really, just use Ubuntu):
- Debian: basically vanilla linux. A lot of other distros are based on debian too, like Kali (for penetration testing), Ubuntu (standard everything), Raspbian (for raspberry pis, a wallet-sized computer)
- Red Hat Enterprise Linux (RHEL): good old tech, proprietary, used by big businesses
- CentOS and Fedora, sort of RHEL, but free

-------------------------------------

also interesting challenge for ppl to think about:

Input (data is 100GB big -> cannot build tree structure):
0:d:b1
1:f:a1
1:f:a2
1:f:a3
1:d:b2
2:f:a4
2:f:a5

Output:
<li>
    b1
    <ul>
        <li>a1</li>
        <li>a2</li>
        <li>a3</li>
        <ul>
            b2
            <li>a4</li>
            <li>a5</li>
        </ul>
    </ul>
</li>

that came up while interviewing dan lmao


*kể tớ chút ít về cậu là ai đi. Cậu học ở đâu, cậu học ngành nào
amherst ntn? vui k?
vì sao cậu chọn CS?
*kể tớ nghe về 1 lần cậu có khó khăn đi. Lúc đó là cậu đang làm gì? Cậu đang làm với ai? Chuyện gì xảy ra? Cậu làm gì để giải quyết nó?
5 năm sau cậu hy vọng cậu sẽ trở thành người như thế nào?
vì sao cậu muốn có công việc này?
what gets you up in the morning?
cậu muốn làm trong team lớn hay team nhỏ
tụi tui có chi nhánh ở gần san francisco, trong bay area. Cậu có muốn làm ở đó k?

cậu bình thường dùng interfaces như thế nào
trong python nó có vụ default arguments, nhưng trong java lại không có. Vậy thì làm sao để tạo ra default arguments trong java?
cậu thấy mình rành những lĩnh vực nào?
tụi tui theo cái scheduling methodology "scrum" và "agile". Không biết là cậu có nghe qua cái đó chưa
cậu có biết gì về dependency injection k?

cậu còn muốn kể tớ cái gì về cậu nữa không?

upbeat vl :v

---------------------------------

for linux sysadmin knowledge, in general

This is what I tell people to do, who ask me "how do I learn to be a Linux sysadmin?".

Set up a KVM hypervisor.

Inside of that KVM hypervisor, install a Spacewalk server. Use CentOS 6 as the distro for all work below. (For bonus points, set up errata importation on the CentOS channels, so you can properly see security update advisory information.)

Create a VM to provide named and dhcpd service to your entire environment. Set up the dhcp daemon to use the Spacewalk server as the pxeboot machine (thus allowing you to use Cobbler to do unattended OS installs). Make sure that every forward zone you create has a reverse zone associated with it. Use something like "internal.virtnet" (but not ".local") as your internal DNS zone.

Use that Spacewalk server to automatically (without touching it) install a new pair of OS instances, with which you will then create a Master/Master pair of LDAP servers. Make sure they register with the Spacewalk server. Do not allow anonymous bind, do not use unencrypted LDAP.

Reconfigure all 3 servers to use LDAP authentication.

Create two new VMs, again unattendedly, which will then be Postgresql VMs. Use pgpool-II to set up master/master replication between them. Export the database from your Spacewalk server and import it into the new pgsql cluster. Reconfigure your Spacewalk instance to run off of that server.

Set up a Puppet Master. Plug it into the Spacewalk server for identifying the inventory it will need to work with. (Cheat and use ansible for deployment purposes, again plugging into the Spacewalk server.)

Deploy another VM. Install iscsitgt and nfs-kernel-server on it. Export a LUN and an NFS share.

Deploy another VM. Install bakula on it, using the postgresql cluster to store its database. Register each machine on it, storing to flatfile. Store the bakula VM's image on the iscsi LUN, and every other machine on the NFS share.

Deploy two more VMs. These will have httpd (Apache2) on them. Leave essentially default for now.

Deploy two more VMs. These will have tomcat on them. Use JBoss Cache to replicate the session caches between them. Use the httpd servers as the frontends for this. The application you will run is JBoss Wiki.

You guessed right, deploy another VM. This will do iptables-based NAT/round-robin loadbalancing between the two httpd servers.

Deploy another VM. On this VM, install postfix. Set it up to use a gmail account to allow you to have it send emails, and receive messages only from your internal network.

Deploy another VM. On this VM, set up a Nagios server. Have it use snmp to monitor the communication state of every relevant service involved above. This means doing a "is the right port open" check, and a "I got the right kind of response" check and "We still have filesystem space free" check.

Deploy another VM. On this VM, set up a syslog daemon to listen to every other server's input. Reconfigure each other server to send their logging output to various files on the syslog server. (For extra credit, set up logstash or kibana or greylog to parse those logs.)

Document every last step you did in getting to this point in your brand new Wiki.

Now go back and create Puppet Manifests to ensure that every last one of these machines is authenticating to the LDAP servers, registered to the Spacewalk server, and backed up by the bakula server.

Now go back, reference your documents, and set up a Puppet Razor profile that hooks into each of these things to allow you to recreate, from scratch, each individual server.

Destroy every secondary machine you've created and use the above profile to recreate them, joining them to the clusters as needed.

Bonus exercise: create three more VMs. A CentOS 5, 6, and 7 machine. On each of these machines, set them up to allow you to create custom RPMs and import them into the Spacewalk server instance. Ensure your Puppet configurations work for all three and produce like-for-like behaviors.

Do these things and you will be fully exposed to every aspect of Linux Enterprise systems administration. Do them well and you will have the technical expertise required to seek "Senior" roles. If you go whole-hog crash-course full-time it with no other means of income, I would expect it would take between 3 and 6 months to go from "I think I'm good with computers" to achieving all of these -- assuming you're not afraid of IRC and google (and have neither friends nor family ...).

--------------------

cloud engineer:

Introduction

So many people struggle with where to get started with AWS and cloud technologies in general. There is popular "How do I learn to be a Linux admin?" post that inspired me to write an equivalent for cloud technologies. This post serves as a guide of goals to grow from basic AWS knowledge to understanding and deploying complex architectures in an automated way. Feel free to pick up where you feel relevant based on prior experience.

Assumptions:

You have basic-to-moderate Linux systems administration skills

You are at least familiar with programming/scripting. You don't need to be a whiz but you should have some decent hands-on experience automating and programming.

You are willing to dedicate the time to overcome complex issues.

You have an AWS Account and a marginal amount of money to spend improving your skills.

How to use this guide:

This is not a step by step how-to guide.

You should take each goal and "figure it out". I have hints to guide you in the right direction.

Google is your friend. AWS Documentation is your friend. Stack Overflow is your friend.

Find out and implement the "right way", not the quick way. Ok, maybe do the quick way first then refactor to the right way before moving on.

Shut down or de-provision as much as you can between learning sessions. You should be able to do everything in this guide for literally less than $50 using the AWS Free Tier. Rebuilding often will reinforce concepts anyway.

Skip ahead and read the Cost Analysis and Automation sections and have them in the back of your mind as you work through the goals.

Lastly, just get hands on, no better time to start then NOW.

Project Overview

This is NOT a guide on how to develop websites on AWS. This uses a website as an excuse to use all the technologies AWS puts at your fingertips. The concepts you will learn going through these exercises apply all over AWS.

This guide takes you through a maturity process from the most basic webpage to an extremely cheap scalable web application. The small app you will build does not matter. It can do anything you want, just keep it simple.

Need an idea? Here: Fortune-of-the-Day - Display a random fortune each page load, have a box at the bottom and a submit button to add a new fortune to the random fortune list.

Account Basics

Create an IAM user for your personal use.

Set up MFA for your root user, turn off all root user API keys.

Set up Billing Alerts for anything over a few dollars.

Configure the AWS CLI for your user using API credentials.

Checkpoint: You can use the AWS CLI to interrogate information about your AWS account.

Web Hosting Basics

Deploy a EC2 VM and host a simple static "Fortune-of-the-Day Coming Soon" web page.

Take a snapshot of your VM, delete the VM, and deploy a new one from the snapshot. Basically disk backup + disk restore.

Checkpoint: You can view a simple HTML page served from your EC2 instance.

Auto Scaling

Create an AMI from that VM and put it in an autoscaling group so one VM always exists.

Put a Elastic Load Balancer infront of that VM and load balance between two Availability Zones (one EC2 in each AZ).

Checkpoint: You can view a simple HTML page served from both of your EC2 instances. You can turn one off and your website is still accessible.

External Data

Create a DynamoDB table and experiment with loading and retrieving data manually, then do the same via a script on your local machine.

Refactor your static page into your Fortune-of-the-Day website (Node, PHP, Python, whatever) which reads/updates a list of fortunes in the AWS DynamoDB table. (Hint: EC2 Instance Role)

Checkpoint: Your HA/AutoScaled website can now load/save data to a database between users and sessions

Web Hosting Platform-as-a-Service

Retire that simple website and re-deploy it on Elastic Beanstalk.

Create a S3 Static Website Bucket, upload some sample static pages/files/images. Add those assets to your Elastic Beanstalk website.

Register a domain (or re-use and existing one). Set Route53 as the Nameservers and use Route53 for DNS. Make www.yourdomain.com go to your Elastic Beanstalk. Make static.yourdomain.com serve data from the S3 bucket.

Enable SSL for your Static S3 Website. This isn't exactly trivial. (Hint: CloudFront + ACM)

Enable SSL for your Elastic Beanstalk Website.

Checkpoint: Your HA/AutoScaled website now serves all data over HTTPS. The same as before, except you don't have to manage the servers, web server software, website deployment, or the load balancer.

Microservices

Refactor your EB website into ONLY providing an API. It should only have a POST/GET to update/retrieve that specific data from DynamoDB. Bonus: Make it a simple REST API. Get rid of www.yourdomain.com and serve this EB as api.yourdomain.com

Move most of the UI piece of your EB website into your Static S3 Website and use Javascript/whatever to retrieve the data from your api.yourdomain.com URL on page load. Send data to the EB URL to have it update the DynamoDB. Get rid of static.yourdomain.com and change your S3 bucket to serve from www.yourdomain.com.

Checkpoint: Your EB deployment is now only a structured way to retrieve data from your database. All of your UI and application logic is served from the S3 Bucket (via CloudFront). You can support many more users since you're no longer using expensive servers to serve your website's static data.

Serverless

Write a AWS Lambda function to email you a list of all of the Fortunes in the DynamoDB table every night. Implement Least Privilege security for the Lambda Role. (Hint: Lambda using Python 3, Boto3, Amazon SES, scheduled with CloudWatch)

Refactor the above app into a Serverless app. This is where it get's a little more abstract and you'll have to do a lot of research, experimentation on your own.

The architecture: Static S3 Website Front-End calls API Gateway which executes a Lambda Function which reads/updates data in the DyanmoDB table.

Use your SSL enabled bucket as the primary domain landing page with static content.

Create an AWS API Gateway, use it to forward HTTP requests to an AWS Lambda function that queries the same data from DynamoDB as your EB Microservice.

Your S3 static content should make Javascript calls to the API Gateway and then update the page with the retrieved data.

Once you have the "Get Fortune" API Gateway + Lambda working, do the "New Fortune" API.

Checkpoint: Your API Gateway and S3 Bucket are fronted by CloudFront with SSL. You have no EC2 instances deployed. All work is done by AWS services and billed as consumed.

Cost Analysis

Explore the AWS pricing models and see how pricing is structured for the services you've used.

Answer the following for each of the main architectures you built:

Roughly how much would this have costed for a month?

How would I scale this architecture and how would my costs change?

Architectures

Basic Web Hosting: HA EC2 Instances Serving Static Web Page behind ELB

Microservices: Elastic Beanstalk SSL Website for only API + S3 Static Website for all static content + DynamoDB Table + Route53 + CloudFront SSL

Serverless: Serverless Website using API Gateway + Lambda Functions + DynamoDB + Route53 + CloudFront SSL + S3 Static Website for all static content

Automation

!!! This is REALLY important !!!

These technologies are the most powerful when they're automated. You can make a Development environment in minutes and experiment and throw it away without a thought. This stuff isn't easy, but it's where the really skilled people excel.

Automate the deployment of the architectures above. Use whatever tool you want. The popular ones are AWS CloudFormation or Teraform. Store your code in AWS CodeCommit or on GitHub. Yes, you can automate the deployment of ALL of the above with native AWS tools.

I suggest when you get each app-related section of the done by hand you go back and automate the provisioning of the infrastructure. For example, automate the provisioning of your EC2 instance. Automate the creation of your S3 Bucket with Static Website Hosting enabled, etc. This is not easy, but it is very rewarding when you see it work.

Continuous Delivery

As you become more familiar with Automating deployments you should explore and implement a Continuous Delivery pipeline.

Develop a CI/CD pipeline to automatically update a dev deployment of your infrastructure when new code is published, and then build a workflow to update the production version if approved. Travis CI is a decent SaaS tool, Jenkins has a huge following too, if you want to stick with AWS-specific technologies you'll be looking at CodePipeline.

Miscellaneous / Bonus

These didn't fit in nicely anywhere but are important AWS topics you should also explore:

IAM: You should really learn how to create complex IAM Policies. You would have had to do basic roles+policies for for the EC2 Instance Role and Lambda Execution Role, but there are many advanced features.

Networking: Create a new VPC from scratch with multiple subnets (you'll learn a LOT of networking concepts), once that is working create another VPC and peer them together. Get a VM in each subnet to talk to eachother using only their private IP addresses.

KMS: Go back and redo the early EC2 instance goals but enable encryption on the disk volumes. Learn how to encrypt an AMI.

Final Thoughts

I've been recently recruiting for Cloud Systems Engineers and Cloud Systems Administrators. We've interviewed over a dozen local people with relevant resume experience. Every single person we interviewed would probably struggle starting with the DynamoDB/AutoScaling work. I'm finding there are very few people that HAVE ACTUALLY DONE THIS STUFF. Many people are familiar with the concepts, but when pushed for details they don't have answers or admit to just peripheral knowledge. You learn SO MUCH by doing.

If you can't find an excuse or get support to do this as part of your job I would find a small but flashy/impressive personal project that you can build and show off as proof of your skills. Open source it on GitHub, make professional documentation, comment as much as is reasonable, and host a demo of the website. Add links to your LinkedIn, reference it on your resume, work it into interview answers, etc. When in a job interview you'll be able to answer all kinds of real-world questions because you've been-there-done-that with most of AWS' major services.

I'm happy to hear any feedback. I'm considering making THIS post my flashy/impressive personal project in the form of a GitHub repo with sample code for each step, architecture diagrams, etc.