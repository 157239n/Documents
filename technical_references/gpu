
nvidia-smi is a pretty nifty tool to inspect nvidia gpus
nvidia-smi -q -g 0 -d UTILIZATION -l
nvidia-smi -l 1
nvidia-smi --gpu-reset -i 0

https://askubuntu.com/questions/887926/no-cuda-capable-device-is-detected-although-requirements-are-installed
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#abstract
https://askubuntu.com/questions/451672/installing-and-testing-cuda-in-ubuntu-14-04
https://registry.hub.docker.com/r/tleyden5iwx/ubuntu-cuda/dockerfile
https://github.com/NVIDIA/nvidia-docker
https://stackoverflow.com/questions/25185405/using-gpu-from-a-docker-container

curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
apt install nvidia-prime

Base image: nvidia/cuda:10.0-base

Kernel: a function that runs in CUDA, access by adding __global__ before the function
Device code: code that runs on the GPU
Host code: code that runs on the CPU

- allocate memory, cudaMallocManaged(), returns a pointer that I can access from host or device code
- free memory, cudaFree()

for cuda version: nvcc --version

1060, $300, 4 teraflops, 6GB, 120W
2080, $700, 8GB, 215W (650W)
2080 Ti, $1000, 14 teraflops, 250W (650W), 12GB
3070, $500, 20 teraflops, 220W (650W), 8GB
3090, $1500, 36 teraflops, 350W (750W), 24GB
Titan V, $3000, 12GB, 110 teraflops, L2 cache 4.6M, 652GB/s, 250W
