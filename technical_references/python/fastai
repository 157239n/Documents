
https://docs.fast.ai/
https://github.com/fastai/fastai
https://github.com/fastai/fastai_dev
https://www.fast.ai/
https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-1-84a1dc2b5236

so, lessions:
1. image classification
2. production; sgd from scratch
3. multi-label; segmentation
4. nlp, tabular data, recsys
5. backprop, neural net from scratch
6. cnn deep dive, ethics
7. resnet, u-net, gans
8. backprop from the foundations
9. the training in depth
10. looking inside the model
11. data block api, generic optimizer
12. advanced training: ulmfit
13. swift: deep learning basics
14. swift: putting it all together

nice link where I dealt with version 0.7:
- https://forums.fast.ai/t/fastai-v0-7-install-issues-thread/24652
- https://forums.fast.ai/t/importerror-cannot-import-name-as-tensor/25295
- https://github.com/NVIDIA/nvidia-docker/issues/864
- https://github.com/codenvy/codenvy/issues/2427
- https://forums.fast.ai/t/trying-to-run-fastai-library-under-virtualenv-hit-bcolz-error/9544/5
- https://forums.fast.ai/t/fastai-v0-7-install-issues-thread/24652

from fastai.basics import *
from fastai.vision import *

dependencies:
core: core -> torch_core ->
	metrics
	layers
	basic_data -> data_block
training: callack -> basic_train -> callbacks -> train
applications:
- collab
- tabular:
		models
		transform -> data
- text:
		transform -> data ------> learn
		models --------------|
- vision:
		image -> transform -> data -----> learn
		models ----------------------|
more details:
- basic_data: wrap [torch.utils.data.Dataset, torch.utils.data.DataLoader] in DeviceDataLoader, in charge of putting the data on the right devices (transformations, like normalization), then regroup them in a DataBunch
- layers: basic functions to define custom layers or groups of layers
- metrics: all the metrics
- callback: 

torch.cuda.get_device_name(0)
torch.cuda.device(0)

ImageDataBunch: https://docs.fast.ai/vision.data.html#ImageDataBunch
- .show_batch(rows=3, figsize=(7,6)), figsize is the size of each samples overall and can be omitted
- .normalize(imagenet_stats), mean 0, sd 1 for channels (0-255 default), imagenet_stats definition appeared here: https://github.com/fastai/fastai/blob/master/fastai/vision/data.py
- .classes: list of classes as strings
- .c: can think of as a count of classes available
- .train_ds
- .valid_ds
	- .x: image file names (absolute paths)
	- .y: labels
- "in fast ai, everything you model with is going to be a data bunch object"
- it contains 2 or 3 datasets: training, validation data, [test data]
- importing data factory methods:
	- from_folder
	- from_csv
	- from_df
	- from_name_re(path_to_images_folder, list_of_file_names, pattern, valid_pct=0.2, ds_tfms=get_transforms()). Sample regular expression: r'/([^/]+)_\d+.jpg$'
	- from_name_func
	- from_lists
	- create_from_ll
	- single_from_classes

ConvLearner:
- "a model is trained in fast ai using something called a learner"
- must feed:
	- data
	- model, or architecture. Samples (https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py):
		- models.resnet34: extremely well, 84MB, nearly all the time (download.pytorch.org/models/resnet34-333f7ec4.pth, https://arxiv.org/pdf/1512.03385.pdf). Others include:
		- resnet18: faster, 44.7MB
		- resnet50
		- resnet101
		- resnet152: more details and hopefully more accuracy
- can feed:
	- metrics, list:
		- error_rate
- is a legacy feature, as i understand it. The new thing is cnn_learner(data, model). Same deal, result is a Learner class, so cnn_learner i guess is the factory?
- .save("stage-1")
- .load("stage-1")
- .fit_one_cycle(1)
- .unfreeze(): train everything, not just the final layers
- .lr_find(): learning rate finder: what's the fastest lr i can do this without sacrificing accuracy
- .recorder.plot() to plot the graph
- .fit_one_cycle(2)
- .export(), create a file named export.pkl
- .predict(image)
- .to_fp16(), switch every operations to operate on 16 bit floating point numbers

so a trained model's weight weighs something in the MB range (around 84 or so). That's insanity. Also resnet34 has already been trained on imagenet data, so it already has a concept of what animals are, and apparently, this pretrained model can work with like 30 sample images, which is insanity to think about

ClassificationInterpretation:
- package: fastai.train
- interp = ClassificationInterpretation.from_learner(learner)
- interp
	- .plot_top_losses(9, figsize=(15,11))
	- .plot_confusion_matrix(figsize=(12,12), dpi=60)
	- .most_confused(min_val=2)
	- .top_losses(), returns (losses, idxs)

FileDeleter:
- FileDeleter(file_paths=top_loss_paths)

bs stands for batch size btw. So more batch size meaning you can crunch through more images per gpu run, meaning the overhead in setting up the gpu environment is reduced

resnet50:
- bs 48 for 11GB gpu
- bs 32 for 8GB gpu
- bs 16 for 4GB gpu

try to mutiply batch size by cycles and see if it adds up to around 1.4k

random functions:
- ds_tfms=get_transforms()
- learn.destroy()
- torch.cuda.empty_cache()
- download_images(path_to_file_with_urls.txt, destination_folder, max_pics=200, max_workers=0), max_workers for not spawning a lot of processes
- verify_images(path_to_folder_with_images, delete=True, max_workers=8)
- np.random.seed(42)
- open_image(path_to_image_file)
- open_mask(path_to_image_file): same as open_image(), but here images are integers

resnet34 reverse engineering
inside binary file:
	conv1.weightq
	bn1.weightq
	bn1.biasq
	layer1.0.bn1.weightq
	layer1.0.bn1.biasq
	layer1.0.conv2.weightq#h
	FloatTensor
	cpuq
	ctorch
	FloatStorage
	jumbled data
	cpuq
	ctorch
	FloatStorage
notes:
	oh, so bn stands for BatchNorm2d
	tops out at 4 layers, which is super weird, because there are something like 7 layers. Anyway, I guess the detailed architecture will be ironed out some time later. 	Now just go with the flow and accept things as is
	stored in a pth file, turns out it's really simple. Just a path contained inside, I guess is where the pythonpath variable should point to? Other times it's a binary file (this case), which again is very interesting

random useful quotes:
- 224x244 is an extremely common size that most models tend to use

DataSet (atomic elements, torch) -> DataLoader (batches, torch) -> DataBunch (binds train_dl and valid_dl, fastai)
"data block api"
ImageFileList.from_folder(path).label_from_csv().random_split_by_pct().datasets().transform().databunch().normalize()
ImageFileList.from_folder(path).label_from_folder().split_by_folder().add_test_folder().datasets().transform(tfms, size=224).databunch()
ImageFileList.from_folder(path).label_from_func().random_split_by_pct().datasets().transform(get_transforms(), size=224).databunch()
things have changed also, so now there are no ImageFileList anymore. But still keep on going through this
.databunch(collate_fn=bb_pad_collate)
TextSplitData.from_csv(...)

for classification, we use this loss function called cross-entropy loss. Can't use this for regression, can only use this on classifications
learner.loss_func = MSELossFlat()

"fit_one_cycle makes the learning rate starts low, go up, and then go down again"

-------

unregulated notes


pytorch, calculating gradient means loss.backward(). But I thought loss is just a number? Okay, so how they did stuff was to declare a as nn.Parameter(a). So this looks like an array wrapper and thus by using this, they can sort of know how to calculate the gradient and whatnot. I sort of couldn't imagined this flexibility a while back, and I still don't know how they did it. So may be this is worthwhile afterall

also if I can just find out where are the damn images, then I probably can develop whole system of commands to clear out unnecessary images or low quality ones, instead of cooperating with the notebook's mechanism

https://forums.fast.ai/t/find-path-to-file-imagedatabunch/50279/6

and the idea to pull live satellite data and look for smoke clouds and whatnot. And may be icebergs too? Just to give out an automatic alert whenever something abnormal is found. I think this would be valuable in terms of creating a learning workflow, and inch me closer to that idea of replacing science's core structure with something like this

"planet amazon dataset"

and may be now make models to predict some physical phenomenon? Like 3 body problem, or other stuff, like pistons and whatnot. Then make these learn on quantum data, which I think will be the nail in the coffin for traditional science

"satellite tiles"

and yeah, can I get data directly from an SSO satellite? Do people publish these data?

"multi-label classification, segmentation"

"a partial"
acc_02 = partial(accuracy_thresh, thresh=0.2)
learner.fit_one_cycle(5, slice(0.01))

wait what? So you are saying that you can just use a model trained on 128x128 images and slap it to a 256x256 image? Don't models need to stay rigid and the same size throughout?

doc_src/data_block

so after going through the sgd thingy they introduced, they kinda still ambiguous about how these architectures work, and how can I adapt this to predict scientific phenomena

image segmentation?

np.loadtxt(path_to_file.txt, dtype=str)

decrease regularization
weight decay
dropout
data augmentation

for segmentation, we can use a cnn, but something else called unet is better

so a cnn will condense things down, then the unet thing expands things out

Learner.create_unet(), so I suspect there will be a Learner.create_cnn() version too? So try this out
learner.recorder.plot_losses()
learner.recorder.plot_lr()

so I suspect that the convolution nn works with sort of an absolute max image size, and every image size below can be applied to here, so that's why you can increase the resolution to double the original (the transfer learning thingy) and it still works. Just my prediction, of course

when we do NLP classification, we actually create 2 models: language model

text_classification_learner()

and it seems like I can actually apply for a grant from drexel to begin doing these things. I can feel these goals are being checked off and can actually be executed. Oh yeah baby, I feel it so strongly it burns. Muahahahaha :v Damn what am I talking about :v Kay bye :v Man, I'm in my prime time right now. I really have to try my best in this small opportunity, and progress even faster than myself in the past

adam, rmsprop

and may be really apply these ML ideas to the scientific backbone, and actually change how science works

how about creating a bunch of images with text inside them, and then draw a boundary layer as the correct answer? Yep this approach is quite interesting, but I have to have the data first to be able to do this

and the tone of them speaking about these things, it's almost as if they have sort of a capability to learn things distributedly, and not relying on a single data center. I also suspect this has to do with transfer learning

df_raw: raw dataset, (401125, 65)
raw_train: (401125-12k)
raw_valid: (12k, 65)
df_trn: converted strings to categories, then into numbers. (401125, 66) (SalePrice gone, <other>_na added)
y_trn: (401125,)
X_train: (401125-12k, 66)
X_valid: (12k, 66)
y_train: (401125-12k,)
y_valid: (12k,)

preds: (40, 12k). 40 for 40 estimators, 12k for predictions of y_valid
preds[:,0]: (40,), predictions of 40 estimators on the first sample of X_valid
preds[:0,]: (0, 12k), predictions of the 1st estimator on every sample
fi: (66, 2), feature importance, has columns col (YearMade, ProductSize) and imp (0.17, 0.04)

conda install matplotlib=2.2.3

collaborative filtering model

pip3 install torch==1.0.1, "Optional" problem, definitely worked on previous ones
but, doesn't solve that mnist problem tho, so try installing a later version
pip3 install torch==1.1.0, "Optional" problem
pip3 install torch==1.2.0, "AttributeError: module 'torch.jit' has no attribute 'unused'" problem
pip3 install torch==1.3.1, "RuntimeError: builtin cannot be used as a value: at /usr/local/lib/python3.6/dist-packages/torchvision/models/detection/_utils.py:14:56" problems
pip3 install torch==1.4.0, problems but only until much later
pip3 install torch==1.2.0 torchvision==0.4.0 pillow==6.1, same problem as 1.4.0, still "IndexError: invalid index to scalar variable."

not reproducible things, after docker restart:
- /usr/local/lib/python3.6/dist-packages/pdpbox/pdp.py, graphing somewhere in lesson 2
after git pull:
- ~/fastai/old/fastai/model.py: lesson 4, at line 243 after np.average() at batch_cnts, removed [0]
