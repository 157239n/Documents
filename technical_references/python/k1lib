
Write in detail what loss landscape actually means

mo:
- tert-butanol
- di-tert-butyl ether
- add parsing support for ketone (2,4-pentadione), carboxylic acid, and aldehydes
- .reorient(), to turn the molecule so that it has nice properties so that we can compare the both of them
- from System, constructs the molecule. This is so that I can save the system to disk and whatnot.

website module automate pull text from sites

io profiler backward shapes

css classes make .Class style
tutorial on language label thingy?

also can just do a normal scheduler, that sets stuff inside Learner itself, cause paramscheduler currently sets param groups' hyperparam.
trigger class, to monitor when the progress flips over to the desired amount

cli accumulate write docs

kxml use css to select out stuff
factor out css selectors?

reorg skin, clarify steps, shoot it. https://www.kaggle.com/shubhamgoel27/dermnet/code
document CoreRNN more, specify expected shapes
document what Learner will deposit into itself when run
create a centralized place to see what Callback uses what fields in Learner
ComputationProfiler make the actual computation calc dependent on a monkey-patched function
do progressive deepening, create blocks in knn and whatnot

make plot names of HookParam and HookModule better. It's not optimal at the moment because the interface for selector is strange and whacky, so fix that first

polyfit more resilient
new function that smoothly fit to data points. "Spline" or sth like that

dl tutorial
callback graph docs generator

HookModule, but actually stores forward and backward passes vanilla. Does not store history

k1lib/tests/moparse.ipynb self-consistency checks failed

graphEqn tackle the full problem

optimization cycles for cli, like in llvm. Learn about llvm again to be sure that it's legit
also for input clis, turn all of them into classes so that `cat("a.txt") | shape(0)` will be rewritten as `None | cmd("wc -l a.txt")`

rethink tee? every feature too. And there's the consume thing from earlier?
split more docs

k1lib.com points to k1lib.github.io, and migrate every link to the private domain slowly?

Add practice probs for cli

callbacks just have an analysis mode that does not keep records

---------------------- cli llvm ----------------------

initially, just a submodule inside main python lib to auto-generate shared library and attach it to the main running process
Also think about whether I can sort of inline the funcs by compiling a whole block of code
You know, if this works then the whole fucking horizontal ai chip future can be leverage much harder and can really pack a punch
Really do the windows build thing, because have to anyway
Concrete goal: read fasta files faster
Also means I can have separate optimizations if I have k1a installed or not

- read java JIT
- see how cython did it
- read pandas source code
- test looping time of real lists and c-yield lists. If roughly the same then can just replace one with the other
- check over ceval a bit this time, to see how control flows typically happen

litmus tests:
- tokenizer

optimizations:

- joinstreams for array just reshapes it
- make it so that (...).all(2) done on a multi-dimensional tensor will automatically resolve to doing the fastest way

---------------------- cli llvm end ----------------------
