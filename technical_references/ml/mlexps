
goals for ML systems going forward?
- document everything as deliverable posts
- recognizes general trends from hyperparam search
- 1 experiment/day seems proper. This will help me to plan for the future as well, and create pressure to make the entire pipeline fast

task: char name classification (50% acc, 0.8 loss is possible)
- bigru approach, understand underlying C++ codebase
task: progressive deepening
- high-res gan stuff. This should verify learning dynamics at later times (think: blind cat)

also, try to apply attention stuff to that gravity prediction problem earlier. Have a great feeling this will work

----------- for later

- migrate blackbox posts to mlexps
- try to do conv5 in blackbox series, so that all linear operations are effectively merged together

rerun char-rnn/5-attn-sizing, so that the graph looks better
rerun imagenet/5-blind-cat, but this time make the image smaller, then bigger, instead of using gaussian blur. Scale factor can be 8x or even 16x

figure out choke limits to vae again, this time for multiple datasets, just to see trends

investigate adversarial attacks again

----------- multinode/disks setups

see if nas system can deliver the performance I need (1 file multi-line access > 1GB/s)
tries to spread data over multiple hdds, then try to read from all of them for multiple processes, just to see what the perf improvements are
do bunch of testing on zfs and applyMp: https://arstechnica.com/information-technology/2020/05/zfs-101-understanding-zfs-storage-and-performance/

so my problem right now is text takes too much time on a single node. So really read over those data books to see what's up and how to scale my infrastructure. I think I can make rapid progress

hadoop distributed file system wiki
https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html
https://en.wikipedia.org/wiki/Distributed_computing
https://en.wikipedia.org/wiki/Apache_Hadoop#HDFS
https://www.youtube.com/watch?v=m9v9lky3zcE&list=PL9ooVrP1hQOEmUPq5vhWfLYJH_b9jFBbR
pytorch distributed data parallel
https://en.m.wikipedia.org/wiki/Comparison_of_distributed_file_systems
https://www.geeksforgeeks.org/what-is-dfsdistributed-file-system/amp/
https://en.m.wikipedia.org/wiki/Distributed_File_System_(Microsoft)
distributed file system
http://ipython.org/ipython-doc/stable/parallel/index.html
https://wiki.python.org/moin/ParallelProcessing
https://docs.ray.io/en/latest/index.html
https://medium.com/distributed-computing-with-ray/how-to-scale-python-multiprocessing-to-a-cluster-with-one-line-of-code-d19f242f60ff
https://stackoverflow.com/questions/5181949/using-the-multiprocessing-module-for-cluster-computing
python multiprocessing multiple nodes

setup k8s to experiment on ray
- https://medium.com/distributed-computing-with-ray/how-to-scale-python-multiprocessing-to-a-cluster-with-one-line-of-code-d19f242f60ff
- https://gist.github.com/edoakes/0f7f62b7d9aa5481482bca23be5f622a
- https://www.anyscale.com/
- https://pages.cs.wisc.edu/~adityav/Evaluation_of_Inter_Process_Communication_Mechanisms.pdf

mlexps make private notebooks

imagenet/video depth: do the predicting distances by being consistent thingy, because it's a real possibility that may reveal more what's really going on

vae vit patch generator

https://huggingface.co/datasets/gigaword
hierarchical softmax: https://towardsdatascience.com/hierarchical-softmax-and-negative-sampling-short-notes-worth-telling-2672010dbe08

that RL course: https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=4&ab_channel=DeepMind

neuflow, dataflow processor: http://yann.lecun.com/exdb/publis/pdf/farabet-ecvw-11.pdf

mlexps send session token over

starting out as a vae, then do a gan. Transition smoothly
freezing subnetworks after vae pretraining, so that they don't go nuts and sort of have to stick with the current implementation
try style transfer, but on a much bigger image
test out diffusion component alone. May be have to do another network where I can feed in random varibles to direct it, for the specific time step component
recognize protein shapes and return similar structures that look kinda the same
