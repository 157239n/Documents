
torch.manual_seed(7)
torch.randn((1, 5))
torch.randn_lile(<torch.Tensor>)
tensor.reshape(1, 3, 5): always new tensor, sometimes same underlying data, sometimes cloned underlying data
tensor.resize_(1, 3, 5): always same tensor, always same underlying data, mutated strides
tensor.view(1, 3, 5): always new tensor, always same underlying data, but check for dimension mismatchs
torch.from_numpy(<np.ndarray>)
torch.numpy()

tensor.mul_(3)

"criterion" = loss function
criterion = nn.CrossEntropyLoss()

optimizer = optim.SGD(model.parameters, lr=0.01)
optimizer.zero_grad()

model.eval(): set model to evaluation mode (dont use dropout)
model.train(): set model to training mode (use dropout)

nn.Dropout
nn.Linear
nn.LogSoftmax
nn.Softmax
nn.ReLU
nn.Sigmoid

torch.save(model.state_dict(), 'checkpoint.pth')
state_dict = torch.load('checkpoint.pth')
model.load_state_dict(state_dict)

torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

for creating a custom dataset, you create a subclass of torch.utils.data.Dataset, implement __len__(self) and __getitem__(seld, idx)

torchvision.models.densenet121(pretrained=True). More here: https://pytorch.org/docs/stable/torchvision/models.html

Module hierarchical structure:
torch.nn.modules.container.Sequential
	[0]: torch.nn.modules.linear.Linear
		.weight: torch.nn.parameter.Parameter
			.data: torch.Tensor
			.grad: torch.Tensor
	[1]: torch.nn.modules.activation.ReLU
	[2]: torch.nn.modules.activation.LogSoftmax

"freeze feature parameters"

Parameter concept explanation
self.linear1 = nn.Linear(in_dim,hid)
self.linear2 = nn.Linear(hid,out_dim)
self.linear1.weight = torch.nn.Parameter(torch.zeros(in_dim,hid))
self.linear1.bias = torch.nn.Parameter(torch.ones(hid))
self.linear2.weight = torch.nn.Parameter(torch.zeros(in_dim,hid))
self.linear2.bias = torch.nn.Parameter(torch.ones(hid))

classifier = nn.Sequential(OrderedDict([
	('fc1', nn.Linear(1024, 500)),
	('relu', nn.ReLU()),
	('fc2', nn.Linear(500, 2)),
	('output', nn.LogSoftmax(dim=1))
]))

model.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
tensor.to(...)

len(dataloader.dataset)

a: batch: (64, 3, 28, 28)
b: single image: (3, 28, 28)
c: a kernel: (3, 4, 4)
	b @ c will produce (26, 26)
	b @ (5 c's) wil produce (5, 26, 26)

"convolutional layer is a stack of feature maps". So I assume these feature maps are rank 2 tensors, and the convolutional layers are rank 3 tensors

x: (1, 1, 213, 320)
conv(): (1, 4, 210, 317)
relu(): (1, 4, 210, 317)
pool(): (1, 4, 105, 158)

conv2d(1, 4, (4, 4))
	1 channel going in. That's the 2nd "1" in x
	4 channels going out. That's the 1st "4" in conv()
	kernel_size is a rank 2 tensor. The vid says that it can be of rank 3, so the dynamics will be a little bit different

the first "1" is the image index in a batch btw. Second is channel number. So it's expected that each "2d" convolution will be convoluted through all channels of the previous layer

pytorch's architecture really, really surprises me, with all the automatic gradient calculation stuff. I used to think it's impossible when doing everything on my own back then. More details:
- https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95
- "how can pytorch calculate derivative"
- https://discuss.pytorch.org/t/calculate-derivative-of-function/57190/6
for later: https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9

torch.nn.utils.clip_grad_norm






torch.cuda.empty_cache()
import gc; gc.collect()

torch.cuda.memory_summary()
tensor.device
tensor.get_device()
tensor.data_ptr()
torch.cuda.memory_snapshot()
torch.cuda.is_available()
torch.cuda.ipc_collect()
torch.get_num_threads()

tensor.save()
tensor.load()




nice reshaping capabilities: https://stackoverflow.com/questions/48686945/reshaping-a-tensor-with-padding-in-pytorch

module.training for evaluation or training mode







pytorch check if fusing happens
- https://pytorch.org/docs/stable/quantization.html
- https://discuss.pytorch.org/t/question-about-quantization-tutorial-and-fusing-model/80652
- https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/

"pytorch profile gpu"
https://pytorch.org/tutorials/recipes/recipes/profiler.html
https://gist.github.com/XinDongol/fe066cb76e1c5238ecbc0cb729806410
https://docs.nvidia.com/deeplearning/frameworks/pyprof-user-guide/profile.html

https://www.kaggle.com/sironghuang/understanding-pytorch-hooks

"pytorch index tensor with range"
https://medium.com/emulation-nerd/every-index-based-operation-youll-ever-need-in-pytorch-a7cef65ea94c
https://stackoverflow.com/questions/61034839/pytorch-indexing-a-range-of-multiple-indices
