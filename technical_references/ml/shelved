
frank hutter, use random forests to actually find optimal hyperparameters. Pretty neat. Generally, the concept is called "automl": https://www.automl.org/book/
bayesian optimization: http://krasserm.github.io/2018/03/21/bayesian-optimization/
on the difficulty of training RNNs: https://arxiv.org/pdf/1211.5063.pdf
batch norm: https://arxiv.org/pdf/1502.03167.pdf
training highly effective connectivities within NNs with randomly initialized, fixed weights: https://arxiv.org/pdf/2006.16627.pdf

deep resources:
- https://pytorch.org/tutorials/
- https://colah.github.io/
- http://jalammar.github.io/
- https://openai.com/progress/
- https://deepmind.com/careers#internships
- clear path: https://openai.com/blog/requests-for-research-2/
- https://spinningup.openai.com/en/latest/
	- https://github.com/openai/spinningup
	- https://github.com/kashif/firedup
	- https://github.com/Kaixhin/spinning-up-basic
	- https://github.com/MishaLaskin/torchingup
	- https://github.com/openai/baselines
	- https://github.com/openai/universe
- deepmind's course: https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF
- trask's book
- human compatible book
- https://www.lesswrong.com/users/nostalgebraist
- grad level lectures: https://www.cs.cornell.edu/courses/cs6787/2017fa/Lecture4.pdf
- https://losslandscape.com/
- https://alignment-newsletter.libsyn.com/
- silver, lecture 4: https://www.youtube.com/watch?v=PnHCvfgC_ZA&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=4
- jeremy, dl: https://www.youtube.com/watch?v=CJKnDu2dxOE
- suggested by openai:
	- deep rl bootcamp: https://sites.google.com/view/deep-rl-bootcamp/lectures
	- CS285 deep rl learning: http://rail.eecs.berkeley.edu/deeprlcourse/
	- david silver: https://www.davidsilver.uk/teaching/
	- random pdf: http://joschu.net/docs/nuts-and-bolts.pdf
	- random pdf: https://github.com/jachiam/rl-intro/blob/master/Presentation/rl_intro.pdf
	- https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html
	- algorithms for RL: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf
	- optimizing expectations: from deep RL to stochastic computational graphs: http://joschu.net/docs/thesis.pdf
	- https://github.com/rll/rllab
	- gym walkthrough: https://gym.openai.com/docs/#review
	- https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/, good q-learning intro
- training RNNs by Sutskever: http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf
- https://aiweirdness.com/
- https://distill.pub/
- https://www.reddit.com/r/artificial/wiki/learning-materials
- https://microscope.openai.com/models
- https://agi.mit.edu
- https://www.youtube.com/channel/UCHB9VepY6kYvZjj0Bgxnpbw/videos
- https://www.youtube.com/c/ArxivInsights/videos
- https://huggingface.co/
- https://bdtechtalks.com/tag/demystifying-ai/
- https://explorabl.es/
- https://www.youtube.com/c/YannicKilcher/videos
- https://gregbrockman.com/
- done:
	- https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html

neuroscience:
- hippocampus doing RL:
	- https://www.nature.com/articles/s41467-019-08998-1
	- https://pubmed.ncbi.nlm.nih.gov/30842581/
	- https://www.frontiersin.org/articles/10.3389/fncom.2016.00128/full
- spiking stuff, direct brain:
	- https://www.nature.com/articles/s41598-020-58831-9.pdf
	- https://singularityhub.com/2020/03/10/scientists-linked-artificial-and-biological-neurons-in-a-network-and-amazingly-it-worked/
- http://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf

GANs:
- original paper: https://arxiv.org/pdf/1406.2661v1.pdf
- dcgan: https://arxiv.org/pdf/1511.06434.pdf
- gan upscaling: http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf
- unsupervised representation learning with deep conv GANs: https://arxiv.org/pdf/1511.06434.pdf, the sunglasses paper
- deep multiscale video prediction beyond mean square error: https://arxiv.org/pdf/1511.05440.pdf
- improved techniques for training GANs: https://arxiv.org/pdf/1606.03498.pdf
- towards principled methods for training GANs: https://arxiv.org/pdf/1701.04862.pdf
- mode collapse:
	- catastrophic forgetting and mode collapse in GANs: https://arxiv.org/pdf/1807.04015.pdf
	- comparison of GAN architectures which reduce mode collapse: https://arxiv.org/pdf/1910.04636.pdf
	- ldmgan: reducing mode collapse in GANs with latent distribution matching: https://openreview.net/pdf?id=HygHbTVYPB
	- veegan: reducing mode collapse in GANs using implicit variational learning: https://michaelgutmann.github.io/assets/papers/Srivastava2017.pdf
	- https://mc.ai/how-to-fight-mode-collapse-in-gans/
	- https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b
	- https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/
	- https://cedar.buffalo.edu/~srihari/CSE676/22.3-GAN%20Mode%20Collapse.pdf
	- https://developers.google.com/machine-learning/gan/problems

super resolution:
- https://arxiv.org/pdf/1609.05158.pdf
- https://arxiv.org/pdf/1707.02921.pdf
- no gans, lots of techniques to unpack: https://towardsdatascience.com/deep-learning-based-super-resolution-without-using-a-gan-11c9bb5b6cd5
- perceptual losses for real-time style transfer and super resolution: https://arxiv.org/pdf/1603.08155.pdf

CNNs:
- trippy images generation:
	- https://www3.cs.stonybrook.edu/~cse352/T12talk.pdf
	- https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html
	- https://arxiv.org/pdf/1412.1897v4.pdf
	- https://arxiv.org/pdf/1412.0035v1.pdf
	- https://arxiv.org/pdf/1506.02753.pdf
	- https://arxiv.org/pdf/1312.6034v2.pdf
- hard negative mining (like negative sampling??):
	- https://www.reddit.com/r/computervision/comments/2ggc5l/what_is_hard_negative_mining_and_how_is_it/
	- https://www.researchgate.net/post/Hard_Negative_Mining_in_CNN_leading_to_class_imbalance
	- https://www.researchgate.net/post/How_to_do_Hard_negative_mining
	- https://openaccess.thecvf.com/content_ECCV_2018/papers/SouYoung_Jin_Unsupervised_Hard-Negative_Mining_ECCV_2018_paper.pdf
	- https://arxiv.org/pdf/1604.03540.pdf
	- https://arxiv.org/pdf/1906.09681.pdf
- detection & segmentation:
	- mask rcnn, like unet, but can differentiate between differences: https://arxiv.org/pdf/1703.06870.pdf
	- ensemble: https://arxiv.org/pdf/1804.09803.pdf
	- object saliency detection, image segmentation: https://arxiv.org/pdf/1505.01173.pdf
	- good end-to-end document recognition: https://dropbox.tech/machine-learning/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning
	- (9/10) selective search for object recognition (R-CNN): https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf
	- RetinaNet: https://arxiv.org/pdf/1708.02002.pdf, https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4

may be unproductive to think about, or ideas that are not of value:

ben goertzel:
- https://goertzel.org/Goertzel_linked_papers_Dec_2014.pdf
- http://www.agi-society.org/journal/
- https://dblp.org/pers/g/Goertzel:Ben.html

opencog:
- https://github.com/opencog
- https://wiki.opencog.org/w/The_Open_Cognition_Project
- https://en.wikipedia.org/wiki/OpenCog
- https://wiki.opencog.org/w/Background_Publications
- https://opencog.org/faq/
- also singularitynet?
- https://ai.stackexchange.com/questions/2381/what-are-general-ideas-behind-opencog
- https://en.wikipedia.org/wiki/Cyc
- trueai.io

this and see over pytorch's c interface and nvidia's interface as well: https://docs.python.org/3/extending/extending.html, https://developer.nvidia.com/how-to-cuda-c-cpp. The goal here is to basically know how the connection works, and to be able to anticipate whether OpenAI can really do stuff like GPT-4, and how can I have control over basically everything in my stack, but still have the flexibility of the python interface. Also see whether I can create a nn.Conv2d closer to what they are actually using. Also I kinda want to create my own stuff in CPU really fast, without going through prebuilt stuff like counter

tensorboard for pytorch? https://pytorch.org/docs/stable/tensorboard.html
