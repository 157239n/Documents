
Write in detail what loss landscape actually means

mo:
- tert-butanol
- di-tert-butyl ether
- add parsing support for ketone (2,4-pentadione), carboxylic acid, and aldehydes
- .reorient(), to turn the molecule so that it has nice properties so that we can compare the both of them
- from System, constructs the molecule. This is so that I can save the system to disk and whatnot.

website module automate pull text from sites

io profiler backward shapes

css classes make .Class style
tutorial on language label thingy?

also can just do a normal scheduler, that sets stuff inside Learner itself, cause paramscheduler currently sets param groups' hyperparam.
trigger class, to monitor when the progress flips over to the desired amount

cli accumulate write docs

kxml use css to select out stuff
factor out css selectors?

reorg skin, clarify steps, shoot it. https://www.kaggle.com/shubhamgoel27/dermnet/code
document CoreRNN more, specify expected shapes
document what Learner will deposit into itself when run
create a centralized place to see what Callback uses what fields in Learner
ComputationProfiler make the actual computation calc dependent on a monkey-patched function
do progressive deepening, create blocks in knn and whatnot

make plot names of HookParam and HookModule better. It's not optimal at the moment because the interface for selector is strange and whacky, so fix that first

polyfit more resilient
new function that smoothly fit to data points. "Spline" or sth like that

dl tutorial
callback graph docs generator

HookModule, but actually stores forward and backward passes vanilla. Does not store history

k1lib/tests/moparse.ipynb self-consistency checks failed

graphEqn tackle the full problem

optimization cycles for cli, like in llvm. Learn about llvm again to be sure that it's legit
also for input clis, turn all of them into classes so that `cat("a.txt") | shape(0)` will be rewritten as `None | cmd("wc -l a.txt")`

proposal: make it so that (...).all(2) done on a multi-dimensional tensor will automatically resolve to doing the fastest way

rethink tee? every feature too. And there's the consume thing from earlier?
split more docs

k1lib.com points to k1lib.github.io, and migrate every link to the private domain slowly?

Add practice probs for cli

---------------------- cli llvm ----------------------

reminders:
- need to consider trace(), as that sort of interacts a lot with apply, aS and friends internally

initially, just a submodule inside main python lib to auto-generate shared library and attach it to the main running process
https://stackoverflow.com/questions/1815812/how-to-create-a-generator-iterator-with-the-python-c-api
Make cat a class (text mode only), so that I can start optimizing it away
So, it's expected that the optimizations are all in a separate folder, containing multiple clis at once
Also extract fastF out into one of these optimization units
May be just before feeding the input iterator in, run some of these optimization passes, and then pass it through
Also think about whether I can sort of inline the funcs by compiling a whole block of code
So, lay the ground work first, then do fastF to refine the interface, then add in optimization units as necessary
You know, if this works then the whole fucking horizontal ai chip future can be leverage much harder and can really pack a punch
Really do the windows build thing, because have to anyway
Concrete goal: read fasta files faster
Also the analysis passes can form a representation of each input and output types
Also means I can have separate optimizations if I have k1a installed or not
Of course, focus on other stuff first, before really over optimizing this, b/c my time is limited

optimizations for after this is implemented:

- joinstreams for array just reshapes it

---------------------- cli llvm end ----------------------
